<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#ff0000"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=apple-touch-icon sizes=180x180 href=/img/favicon/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/img/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/img/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=384x384 href=/img/favicon/android-chrome-384x384.png><link rel=icon type=image/png sizes=192x192 href=/img/favicon/android-chrome-192x192.png><link rel=icon type=image/png sizes=150x150 href=/img/favicon/mstile-150x150.png><link rel="shortcut icon" href=/favicon.ico><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/img/favicon/safari-pinned-tab.svg color=#ce2949><link rel=schema.dc href=http://purl.org/DC/elements/1.0/><meta name=citation_public_url content="https://startwords.cdh.princeton.edu/issues/1/data-beyond-vision/"><meta name=citation_title content="Data Beyond Vision"><meta name=citation_date content="2020/10"><meta name=citation_author content="Rebecca Sutton Koeser"><meta name=citation_author content="Gissoo Doroudian"><meta name=citation_author content="Nick Budak"><meta name=citation_author content="Xinyi Li"><meta name=citation_pdf_url content="https://zenodo.org/record/4139781/files/startwords-1-data-beyond-vision.pdf"><meta name=citation_doi content="10.5281/zenodo.3713670"><meta name=citation_abstract content="How do we represent tangible objects in a visual medium? We use words, pictures, and diagrams. We describe, share, show, and fail."><meta name=citation_journal_title content="Startwords"><meta name=citation_issn content="2694-2658"><meta name=citation_issue content="1"><meta name=citation_publisher content="Center for Digital Humanities, Princeton University"><meta name=DC.rights content="http://creativecommons.org/licenses/by/4.0/"><meta name=author content="Rebecca Sutton Koeser, Gissoo Doroudian, Nick Budak, Xinyi Li"><meta name=generator content="Center for Digital Humanities, Princeton University"><meta name=dcterms.created content="2020-10"><link rel=preload as=font type=font/woff2 href=/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-regular.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/Source_Sans_Pro/source-sans-pro-v13-latin-regular.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/Source_Sans_Pro/source-sans-pro-v13-latin-italic.woff2 crossorigin><link rel=preconnect href=https://cdn.jsdelivr.net><link rel=preconnect href=https://static.sketchfab.com><link rel=preconnect href=https://media.sketchfab.com><title>Data Beyond Vision</title><meta name=description content="Startwords Issue 1, October 2020. A research periodical irregularly published by the Center for Digital Humanities at Princeton."><meta property="og:title" content="Data Beyond Vision"><meta property="og:description" content="How do we represent tangible objects in a visual medium? We use words, pictures, and diagrams. We describe, share, show, and fail."><meta property="og:type" content="article"><meta property="og:url" content="https://startwords.cdh.princeton.edu/issues/1/data-beyond-vision/"><meta property="og:image" content="https://startwords.cdh.princeton.edu/issues/1/data-beyond-vision/images/dbv-social.jpg"><meta property="article:published_time" content="2020-10-01T00:00:00+00:00"><meta property="article:modified_time" content="2020-10-27T12:17:43-04:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://startwords.cdh.princeton.edu/issues/1/data-beyond-vision/images/dbv-social.jpg"><meta name=twitter:title content="Data Beyond Vision"><meta name=twitter:description content="How do we represent tangible objects in a visual medium? We use words, pictures, and diagrams. We describe, share, show, and fail."><link rel=stylesheet href=/style.css><link rel=stylesheet href=/print.css media=print><link rel=stylesheet href=/issues/1/data-beyond-vision/style.css><script src=/js/polyfills.js></script><script defer src=/js/bundle.js></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-87887700-8','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script defer src=https://cdn.jsdelivr.net/npm/openseadragon@2.4.2/build/openseadragon/openseadragon.min.js integrity="sha256-NMxPj6Qf1CWCzNQfKoFU8Jx18ToY4OWgnUO1cJWTWuw=" crossorigin=anonymous></script><link rel=alternate type=text/plain href=https://startwords.cdh.princeton.edu/issues/1/data-beyond-vision/index.txt></head><body class=article><header><nav aria-label=main><ul><li class=home><a href=/><img class=logo src=/img/logos/startwords.svg alt=Home></a></li><li class=issues><a href=/issues><span>Issues</span></a></li></ul></nav></header><main><article><div class=grid><header><p class=number><a href=/issues/1/>Issue 1</a></p><p class=theme>Transformations</p><h1>Data Beyond Vision</h1><ul class=authors><li><address>Rebecca Sutton Koeser</address></li><li><address>Gissoo Doroudian</address></li><li><address>Nick Budak</address></li><li><address>Xinyi Li</address></li></ul><br><time class=pubdate datetime=2020-10>October 2020</time><br><ul class=tags><li><span class=tag>DataBeyondVision</span></li></ul><br><a href=http://doi.org/10.5281/zenodo.3713670 rel=alternate class=doi>10.5281/zenodo.3713670</a><br><p class=formats><a href=/issues/1/data-beyond-vision/index.txt rel=alternate type=text/plain>TXT</a>
<a href=https://zenodo.org/record/4139781/files/startwords-1-data-beyond-vision.pdf rel=alternate type=application/pdf>PDF</a></p></header><section class=print-only><a class=first-page-header href=/ aria-label=Startwords><img alt=Startwords src=/pdf-logotype.svg></a>
<a class=page-header href=/ aria-label=Startwords><img alt=Startwords src=/pdf-logo.svg></a>
<a href=http://doi.org/10.5281/zenodo.3713670 rel=alternate class=page-doi>doi:10.5281/zenodo.3713670</a></section><div class=text-container><p>How do we represent tangible objects in a visual medium? We use words, pictures, and diagrams. We describe, share, show, and fail.</p><p>Humanists continue to expand the range of objects they study, but the range of scholarly outputs has not seen a similar expansion. While there are movements within Digital Humanities to consider nontraditional formats, the presentation and publishing of these experimental works (such as installations and project demos) are still secondary or sidelined, where they exist at all. What would it look like to consider non-textual research outputs as first-order scholarly work? The historian David Staley suggests the terms “interpretive objects” or “humanistic objects” for creative scholarly acts that are not limited to text;<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> Catherine D’Ignazio and Lauren Klein offer the broader term “rhetorical objects.”<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> This kind of innovative work is carefully researched and theorized. It deserves scholarly engagement and intellectual rigor even if it does not fit into established modes of scholarly communication.</p><p>Academic research has a long history of textual practice and citation that we haven’t yet figured out how to adapt to non-textual scholarship. Both the <a href=https://openhumanitiesdata.metajnl.com/>Journal of Open Humanities Data</a> and the <a href=https://joss.theoj.org/>Journal of Open Source Software</a> can be seen as steps in this direction: they provide venues for the review and publication of data and software, respectively, accompanied by brief “metapapers.” But even these journals rely on transforming the content they review—data and software—into text in order to function! What are the implications if we truly expand the range of accepted scholarly outputs to include such interpretive objects as data structures, databases, software, datasets, physical objects, and augmented reality experiences? Will scholars need to become experts in all these modes, or can we find a way to become conversant in multiple forms of argumentation, as with other important scholarly theories?</p><p>The pieces that follow describe four different data physicalizations, which we consider to be one class of interpretive object. This is an exploration of our work as we wrestle with how to present physical objects in a non-physical medium, objects meant to be held, touched, or viewed from different angles. Not quite metapaper, manual, or manifesto — these are guides toward reading and thinking in creative new scholarly modes.<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup></p><div id=interlude><div class=left><p><strong>SEE</strong></p><p>from a distance.<br>Cold, commanding.<br>Sense of mastery,<br>but optical illusions deceive.</p><p>Look in a mirror.<br>and see yourself<br>seeing.</p></div><div class=right><p><strong>TOUCH</strong></p><p>up close.<br>Intimate, incomplete.<br>Explore partial knowledge,<br>enlighten slowly.</p><p>Run fingers across skin<br>and touch yourself<br>touching.</p></div></div><p>Data physicalization represents data in physical form.<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> Like other approaches to understanding and representing data, it highlights particular senses to communicate information, specifically touch and sight.<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup> But data physicalization is distinct from other sensory approaches in that it bridges the gap between creative, physical, and conceptual exploration, a nexus often associated with critical making.<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup> Data physicalizations surface the amount of labor involved with data production and representation; they lend data different perspectives and dimensions.<sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup> These physicalizations also create an opportunity for viewers to become active participants in the making of a piece using data.</p><p>Data physicalization attempts to defamiliarize us from the many two-dimensional data representations we have seen by literally placing data in the <em>mise en scène</em> of a conceptual exploration. There is something unique about turning data points into physical forms and placing them in space, something that triggers the mind to understand data in a distinctive way.</p><figure aria-describedby=conceptmap-desc><img loading=lazy alt="Diagram defining the relationships between various data representations and the senses they incorporate; content available in description" src=/issues/1/data-beyond-vision/images/conceptmap.svg role=img><figcaption><p>Concept map situating data physicalization in relation to other types of data representations and interpretations. Revised from the concept map included in the poster presented by the authors at DH2019 in Utrecht. Rebecca Sutton Koeser, Nick Budak, Gissoo Doroudian, and Xinyi Li, “Data Beyond Vision” (poster, DH2019, Utrecht, Netherlands, July 11, 2019).</p></figcaption></figure><div class=txt-only>⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩
| FIGURE.
|
| CAPTION: Concept map situating data physicalization in relation to other types of data representations and interpretations. Revised from the concept map included in the poster presented by the authors at DH2019 in Utrecht.
| ATTRIBUTION: Koeser, Rebecca Sutton, Nick Budak, Gissoo Doroudian, and Xinyi Li. “Data Beyond Vision,” July 11, 2019.
| LINK: <a href=https://doi.org/10.5281/zenodo.3261531>https://doi.org/10.5281/zenodo.3261531</a>
⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩</div><div class=sr-only id=conceptmap-desc><p>Other approaches for data representation and interpretation include:</p><ul><li>Data Visualization, which focuses on storytelling by using graphical elements</li><li>Data Edibilization, which focuses on experiencing data through food using edible materials</li><li>Data Sonification, which focuses on auditory patterns by using sound</li><li>Data Visceralization, which focuses on physical and emotional experience by using multiple senses and affect, making it the only approach that emphasizes emotion.</li><li>Data Art, which focuses on representing links between data and artistic creations by using expressive frameworks and raw data.</li><li>Interpretive object, which focuses on revealing meanings and relationships via non-textual forms by using metaphors.</li></ul></div><p>There is an ethics of drawing on other senses. Feminist philosopher Donna Haraway describes “visualizing technologies” as the “god trick of seeing everything from nowhere.”<sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup> Klein and D’Ignazio expand on this, demonstrating that the assumed neutrality and objectivity of even the simplest visualizations always come from a particular perspective, usually a dominant cultural view that fundamentally excludes and marginalizes.<sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup></p><p>This is especially the case when making data visualizations accessible to vision-impaired readers. The typical solution is to provide a table with the data underlying the chart or graph. This isn’t practical for large datasets, and it’s clearly not the same experience; otherwise, we would provide the tabular data to all users. Another approach is to provide an extended description sharing the insights gained from the chart.<sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup> This is helpful, but pre-digesting the chart in this way doesn’t allow readers to perceive and interpret the patterns and draw their own conclusions. Tactile data physicalizations provide sensory forms that offer individuals the opportunity to explore and discover patterns in the data for themselves.</p><p>Approaching an object like a data physicalization on display encourages bodily engagement in physical space. It encourages the person encountering it to consider multiple angles and perspectives, and it should raise questions about how the objects are meant to be read. This touching requires proximity and a certain intimacy.<sup id=fnref:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup> For Emmanuel Levinas, the “ocularcentrism” of western civilization has produced a false sense that vision is synonymous with objectivity; he proposes instead the metaphor of touch as the basis for ethical engagement with the Other. Ocularcentrism requires distance or separation and encourages objectification and mastery of that which is viewed.<sup id=fnref:12><a href=#fn:12 class=footnote-ref role=doc-noteref>12</a></sup> French feminist philosopher Luce Irigaray extends this metaphor in her notion of the “caress,” which “weds without consum(mat)ing.”<sup id=fnref:13><a href=#fn:13 class=footnote-ref role=doc-noteref>13</a></sup> Because touch requires intimacy, boundaries, and consent, it offers connection without the taint of mastery.<sup id=fnref:14><a href=#fn:14 class=footnote-ref role=doc-noteref>14</a></sup></p><hr><p>These are not artist statements because this is not art; these objects might look like Data Art, but the goals and methods are different. This is representation, correspondence, laborious translation. These are our attempts to communicate our goals, to help you to <em>read</em> and interpret these unfamiliar objects, and to be challenged by the potential of physicalization.</p><p>We invite you to participate in the embodiment and visible labor of data work. Download the following models and instructions, use your hands to recreate the data physicalizations we developed, or use them as inspiration to make your own interpretive objects. If you make any of these physicalizations, please share them on social media with the hashtag <a href="https://twitter.com/search?q=(%23DataBeyondVision)">#DataBeyondVision</a>.</p><div class=icon-nav><span class=help>Choose an object</span>
<a href=#folding><img src=images/icon-folding.svg alt="read folding section"></a>
<a href=#modeling><img src=images/icon-printing.svg alt="read modeling section"></a>
<a href=#weaving><img src=images/icon-weaving.svg alt="read weaving section"></a>
<a href=#stacking><img src=images/icon-stacking.svg alt="read stacking section"></a></div><h2 id=folding>Folding in the Lesser Known Members of the Shakespeare and Company Lending Library</h2><p><em>Nick Budak, Xinyi Li</em></p><h3 id=goal>Goal</h3><p>The Shakespeare and Company lending library is best known for its famous members — writers such as Gertrude Stein, James Joyce, Ernest Hemingway, Aimé Césaire, and Simone de Beauvoir. We wanted to highlight the activity of the relatively unknown members — many of them women — who in fact represent a much larger portion of the library&rsquo;s day-to-day activity and thus arguably better represent it than do the prominent names. This piece makes use of unit origami to create a larger, cohesive form from small folded units, mirroring the relationship between the overall membership of the library and a single member.<sup id=fnref:15><a href=#fn:15 class=footnote-ref role=doc-noteref>15</a></sup></p><figure><img loading=lazy alt="Folded origami model of a green cube intersecting a white octahedron covered with printed text." src=/issues/1/data-beyond-vision/images/folding-installation-photo.jpg sizes="(max-width: 768px) 100vw, 80vw" srcset="/issues/1/data-beyond-vision/images/folding-installation-photo_hue8cc9d37f77dd4cd10109019f3675f97_9534334_500x0_resize_q75_box.jpg 500w,
/issues/1/data-beyond-vision/images/folding-installation-photo_hue8cc9d37f77dd4cd10109019f3675f97_9534334_800x0_resize_q75_box.jpg 800w,
/issues/1/data-beyond-vision/images/folding-installation-photo_hue8cc9d37f77dd4cd10109019f3675f97_9534334_1200x0_resize_q75_box.jpg 1200w,
/issues/1/data-beyond-vision/images/folding-installation-photo_hue8cc9d37f77dd4cd10109019f3675f97_9534334_1500x0_resize_q75_box.jpg 1500w,
/issues/1/data-beyond-vision/images/folding-installation-photo_hue8cc9d37f77dd4cd10109019f3675f97_9534334_1800x0_resize_q75_box.jpg 1800w,
/issues/1/data-beyond-vision/images/folding-installation-photo.jpg 5472w" class=landscape><figcaption><p>Completed piece on display at the Center for Digital Humanities, with early drafts visible in background.</p></figcaption></figure><div class=txt-only>⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩
| FIGURE. Folded origami model of a green cube intersecting a white octahedron covered with printed text.
|
| CAPTION: Completed piece on display at the Center for Digital Humanities, with early drafts visible in background.
| ATTRIBUTION: Photo by Shelley Szwast.
⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩</div><h3 id=description>Description</h3><p>The physicalization contrasts the activity of the better known members of the lending library — those linked by researchers to an entry in the <a href=https://viaf.org/>Virtual International Authority File</a> (VIAF)<sup id=fnref:16><a href=#fn:16 class=footnote-ref role=doc-noteref>16</a></sup> — with the activity of relatively unknown members with no known authority record.<sup id=fnref:17><a href=#fn:17 class=footnote-ref role=doc-noteref>17</a></sup> Activity is represented by the total number of borrowing events that would plausibly have brought members into the library, namely checking out and returning books. Names of the lesser known members are printed on the paper used to create the octahedron as a way of corporealizing and “re-humanizing” humanities data.<sup id=fnref:18><a href=#fn:18 class=footnote-ref role=doc-noteref>18</a></sup> By holding the physicalization in two different ways, the user can “grasp” two separate sets of data: the octahedron (non-famous members) and the cube (famous members). The ratio of the volumes of these two solids reflects the use of the library by these two different groups.<sup id=fnref:19><a href=#fn:19 class=footnote-ref role=doc-noteref>19</a></sup></p><p><div class=sketchfab-embed-wrapper><iframe id=sketchfab-9c96fadd27c34a11902f0a1281ea0ab4 aria-label="Two different still images of the model rotated in the 3D viewer, emphasizing the two shapes combined in the object: the cube and the octahedron." frameborder=0 allow="autoplay; fullscreen; vr" mozallowfullscreen=true webkitallowfullscreen=true loading=lazy src="https://sketchfab.com/models/9c96fadd27c34a11902f0a1281ea0ab4/embed?autospin=0.2&autostart=0&camera=0&preload=1&ui_controls=1&ui_infos=1&ui_inspector=1&ui_stop=1&ui_watermark=1&ui_watermark_link=1"></iframe></div><div class=txt-only>⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩
| FIGURE. 3-D model of a green cube intersecting a white octahedron covered with printed text.
⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩</div></p><h3 id=insights>Insights</h3><figure aria-describedby=pie-desc><img loading=lazy alt="A pie chart with a 70% majority section labeled “no VIAF” in green, with the remainder labeled “VIAF”." style=max-height:300px src=/issues/1/data-beyond-vision/images/folding_viaf_pie.svg role=img><figcaption><p>A pie chart representing the proportions of members with and without VIAF identification.</p></figcaption></figure><div class=txt-only>⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩
| FIGURE. A pie chart with a 70% majority section labeled “no VIAF” in green, with the remainder labeled “VIAF”.
|
| CAPTION: A pie chart representing the proportions of members with and without VIAF identification.
⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩</div><div class=sr-only id=pie-desc>The majority section of the pie chart represents the 14,583 library members who we identified as “lesser known” because they are not listed in VIAF. The minority section represents the 6,248 members who are listed in VIAF, often because they were involved with a well-known creative work.</div><p>A pie chart can be used to present the same ratio of data conveyed in the physicalization; this representation is useful when we want to illustrate a situation where we know the totality of the data. Pie charts also depict a world where data fit neatly into mutually exclusive categories. The act of grasping the two intersecting solids in our physicalization is a response to this approach: the membership data of the lending library is a work in progress, updated as researchers comb through archives that are fragmentary and incomplete. One cannot see all sides of a three-dimensional solid simultaneously. By representing our data as intersecting solids, we instead mirror the fuzzy distinction between “famous” and “non-famous” members, acknowledging the intersection of the varied identities of the library’s members.<sup id=fnref:20><a href=#fn:20 class=footnote-ref role=doc-noteref>20</a></sup></p><h3 id=next-steps>Next Steps</h3><p>Using cut, punched, or embossed paper would make the piece more tactile; instead of simply printing names, we could add unique patterns to represent membership and borrowing activities for individual members. In the future, we could use generative methods to create unique folding patterns for individual library member activity and make them available via print-on-demand. This would enable viewers to become participants and turn folding into an act of recovery of the stories of the lesser known library members.<sup id=fnref:21><a href=#fn:21 class=footnote-ref role=doc-noteref>21</a></sup></p><div class=icon-nav><span class=help>Choose a different object</span>
<a href=#modeling><img src=images/icon-printing.svg alt="read modeling section"></a>
<a href=#weaving><img src=images/icon-weaving.svg alt="read weaving section"></a>
<a href=#stacking><img src=images/icon-stacking.svg alt="read stacking section"></a></div><h2 id=modeling>Modeling Shakespeare and Company Library Membership</h2><p><em>Rebecca Sutton Koeser</em></p><h3 id=goal-1>Goal</h3><figure><img loading=lazy alt="3D printed object and accompanying 3D printed labels laid out on a table; this side view shows labels for the years 1919–1942." src=/issues/1/data-beyond-vision/images/modeling-side-view.jpg sizes="(max-width: 768px) 100vw, 80vw" srcset="/issues/1/data-beyond-vision/images/modeling-side-view_hu5bc641453e1033078d4f48224daca5b0_4425375_500x0_resize_q75_box.jpg 500w,
/issues/1/data-beyond-vision/images/modeling-side-view_hu5bc641453e1033078d4f48224daca5b0_4425375_800x0_resize_q75_box.jpg 800w,
/issues/1/data-beyond-vision/images/modeling-side-view_hu5bc641453e1033078d4f48224daca5b0_4425375_1200x0_resize_q75_box.jpg 1200w,
/issues/1/data-beyond-vision/images/modeling-side-view_hu5bc641453e1033078d4f48224daca5b0_4425375_1500x0_resize_q75_box.jpg 1500w,
/issues/1/data-beyond-vision/images/modeling-side-view_hu5bc641453e1033078d4f48224daca5b0_4425375_1800x0_resize_q75_box.jpg 1800w,
/issues/1/data-beyond-vision/images/modeling-side-view.jpg 3400w" class=landscape><figcaption><p>Side view of 3D printed lollipop chart with labels and statement.</p></figcaption></figure><div class=txt-only>⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩
| FIGURE. 3D printed object and accompanying 3D printed labels laid out on a table; this side view shows labels for the years 1919–1942.
|
| CAPTION: Side view of 3D printed lollipop chart with labels and statement.
| ATTRIBUTION: Photo by Shelley Szwast.
⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩</div><p>This data physicalization demonstrates the affordances of three dimensions for representing data: time series data are displayed with sequential months and years adjacent to each other, which makes it easier to discern seasonal and annual trends. I hope to inspire others to try experimental approaches to representing data; writing software to generate printable 3D models directly from the data makes the process reproducible, and may eventually enable others to create and print their own physicalizations. The tactile nature of the object suggests the possibilities of 3D printing to create more accessible representations of data.</p><figure><img loading=lazy alt="Alternating rows of white and green “lollipops” fade into the distance and out of focus, with the white data points noticeably larger in the foreground." src=/issues/1/data-beyond-vision/images/modeling-close-up.jpg sizes="(max-width: 768px) 100vw, 80vw" srcset="/issues/1/data-beyond-vision/images/modeling-close-up_hub2dfc5608e8305f7eaaaae3e90a5c8a5_5473478_500x0_resize_q75_box.jpg 500w,
/issues/1/data-beyond-vision/images/modeling-close-up_hub2dfc5608e8305f7eaaaae3e90a5c8a5_5473478_800x0_resize_q75_box.jpg 800w,
/issues/1/data-beyond-vision/images/modeling-close-up_hub2dfc5608e8305f7eaaaae3e90a5c8a5_5473478_1200x0_resize_q75_box.jpg 1200w,
/issues/1/data-beyond-vision/images/modeling-close-up_hub2dfc5608e8305f7eaaaae3e90a5c8a5_5473478_1500x0_resize_q75_box.jpg 1500w,
/issues/1/data-beyond-vision/images/modeling-close-up_hub2dfc5608e8305f7eaaaae3e90a5c8a5_5473478_1800x0_resize_q75_box.jpg 1800w,
/issues/1/data-beyond-vision/images/modeling-close-up.jpg 3400w" class=landscape><figcaption><p>Close up of 3D printed lollipop chart with labels.</p></figcaption></figure><div class=txt-only>⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩
| FIGURE. Alternating rows of white and green “lollipops” fade into the distance and out of focus, with the white data points noticeably larger in the foreground.
|
| CAPTION: Close-up of 3D printed lollipop chart with labels.
| ATTRIBUTION: Photo by Shelley Szwast.
⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩</div><h3 id=description-1>Description</h3><p>This is a two-variable, three-dimensional lollipop chart showing the membership of the Shakespeare and Company lending library and bookshop, by month and year, from November 1919 when Sylvia Beach opened her bookshop to its official closing in 1941.<sup id=fnref:22><a href=#fn:22 class=footnote-ref role=doc-noteref>22</a></sup> Membership data are drawn from two different sources, both of which are incomplete: broad membership information comes from <a href=https://shakespeareandco.princeton.edu/sources/logbooks/>logbooks</a> (although logbooks for 1930, parts of 1931–32, and 1937 are missing); detailed borrowing histories come from <a href=https://shakespeareandco.princeton.edu/sources/cards/>lending library cards</a> for a subset of members.<sup id=fnref:23><a href=#fn:23 class=footnote-ref role=doc-noteref>23</a></sup> The white octahedrons represent the number of members with an active membership in each month; the green icospheres correspond to the number of members with borrowing activity in each month.<sup id=fnref:24><a href=#fn:24 class=footnote-ref role=doc-noteref>24</a></sup> For any month where the value is zero, there is no lollipop. Representing the two different datasets as adjacent, half lollipops exposes the discrepancies between the stories these sources tell us about the membership of the library without privileging either of them.<sup id=fnref:25><a href=#fn:25 class=footnote-ref role=doc-noteref>25</a></sup> Using two different shapes makes the two parts of the physicalization distinguishable to touch. The two lollipop charts are designed to be printed independently and then assembled, so that any 3D printer can be used. In this version, the two pieces slide together; this is both a simplification and an improvement over the previous version, where one piece was placed on top of the other.<sup id=fnref:26><a href=#fn:26 class=footnote-ref role=doc-noteref>26</a></sup></p><p><div class=sketchfab-embed-wrapper><iframe id=sketchfab-89985d66f7244d87b7edbe5fd6266f0d aria-label="3-D model of Shakespeare and Company membership lollipop chart." frameborder=0 allow="autoplay; fullscreen; vr" mozallowfullscreen=true webkitallowfullscreen=true loading=lazy src="https://sketchfab.com/models/89985d66f7244d87b7edbe5fd6266f0d/embed?autospin=0.2&autostart=0&camera=0&preload=1&ui_controls=1&ui_infos=1&ui_inspector=1&ui_stop=1&ui_watermark=1&ui_watermark_link=1"></iframe><figure class=preview><img src=images/modeling-3d-alt.jpg alt="3D printed object and accompanying 3D printed labels displayed on a table; this side view shows labels for the years 1919–1942."><figcaption><p>The online version of this essay includes an interactive 3D viewer displaying a model of this object.</p></figcaption></figure></div><div class=txt-only>⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩
| FIGURE. 3-D model of Shakespeare and Company membership lollipop chart.
⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩</div></p><h3 id=insights-1>Insights</h3><figure aria-describedby=bars-desc><img loading=lazy alt="Bar chart showing members with borrowing activity and total members each month" src=/issues/1/data-beyond-vision/images/membership-book-activity-combined_1919-19411.svg role=img><figcaption><p>Shakespeare and Company lending library members with borrowing activity and members, 1919–1941. From Kotin and Koeser, “Shakespeare and Company Lending Library Cards.”</p></figcaption></figure><div class=txt-only>⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩
| FIGURE. Bar chart showing members with borrowing activity and total members each month.
|
| CAPTION: Shakespeare and Company lending library members with borrowing activity and members, 1919–1941.
| ATTRIBUTION: Kotin and Koeser, “The Shakespeare and Company Lending Library Cards in Context.”
| LINK: <a href=http://shakespeareandco.princeton.edu/analysis/2020/03/shakespeare-and-company-lending-library-cards-context/>http://shakespeareandco.princeton.edu/analysis/2020/03/shakespeare-and-company-lending-library-cards-context/</a>
⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩</div><div class=sr-only id=bars-desc>Plotting documented membership accounts against borrowing activity shows the differences in members tracked across the two sources. The Shakespeare and Company Project has information about the borrowing activity of 11% of lending library members. For the 1920s, the percentage is lower: only 6%. But in the 1930s, the percentage is higher: 23%. Some months show more members with borrowing activity than total members because information on the cards overlaps gaps in logbook coverage.</div><p>The same membership data can be presented in a two-variable bar chart. Overall trends are easy to see, and both representations of the data make it possible to compare the two data series against each other. Seasonal trends are visible in the bar chart, but it’s difficult to identify distinct months; in contrast, changing perspective on the 3D lollipop chart allows us to focus on yearly or monthly trends. Missing data in one variable are visible in both, but seem more striking in the 3D version where the base of the piece is bare without any lollipop tops. At the current scale, touching the piece requires focusing attention on just a portion of the object but invites exploration, which can proceed in any direction.<sup id=fnref:27><a href=#fn:27 class=footnote-ref role=doc-noteref>27</a></sup> While the bar chart demands sequential reading from left to right, the 3D printed object doesn’t provide or demand a particular starting point or sequence.</p><p>The bar chart conveys a sense of certitude and exactness that does not reflect the missing and partial data that underlie it; the 3D printed object, with its irregularities and fragility, is more representative of the contingent, historical data.<sup id=fnref:28><a href=#fn:28 class=footnote-ref role=doc-noteref>28</a></sup></p><h3 id=next-steps-1>Next Steps</h3><p>The current version uses different shapes for the two variables, but adding textures would make the model even more tactile. Simple 3D printed labels with text and braille have been added for display alongside the piece, but they could be incorporated directly on the model, and refined to provide a scale for the axes.<sup id=fnref:29><a href=#fn:29 class=footnote-ref role=doc-noteref>29</a></sup> The 3D printed objects could also be augmented with other media: lights or sound could convey the intensity of borrowing activity, or threads connecting months could represent the number of subscription renewals and convey a sense of continuity. The Python code used to create these models could be generalized for reuse, and eventually made available as a Blender plugin.<sup id=fnref:30><a href=#fn:30 class=footnote-ref role=doc-noteref>30</a></sup> New shapes or approaches could be used to leverage innovations in 3D modeling, such as generative design, to create objects that are more inviting to touch and even more distinct from 2D data visualizations. 3D models could be revised for fabrication with CNC machines to create objects out of wood instead of plastic, which could make them more inviting to touch.</p><p><a href=/issues/1/modeling-howto/>Make one ></a></p><div class=icon-nav><span class=help>Choose a different object</span>
<a href=#folding><img src=images/icon-folding.svg alt="read folding section"></a>
<a href=#weaving><img src=images/icon-weaving.svg alt="read weaving section"></a>
<a href=#stacking><img src=images/icon-stacking.svg alt="read stacking section"></a></div><h2 id=weaving>Weaving Derrida’s References</h2><blockquote><p>… we all of us, grave or light, get our thoughts entangled in metaphors…
<cite>George Eliot, Middlemarch</cite></p></blockquote><p><em>Rebecca Sutton Koeser, Gissoo Doroudian</em></p><h3 id=goal-2>Goal</h3><p>With this piece, we aim to literalize the metaphor of weaving as writing, embedded in the very words “textile” and “text,” by representing Derrida’s intertextuality as a woven tapestry.<sup id=fnref:31><a href=#fn:31 class=footnote-ref role=doc-noteref>31</a></sup> The textures of the yarn and woven fabric invite touch, but by showing an in-progress weaving with the pattern and instructions provided, we move viewers beyond seeing and touching to enable them to become participants in reconstructing the data. Showing the weaving in progress also foregrounds the labor of data work, since curation, collection, and visualization all take an enormous amount of work and skill, often from a range of different individuals.</p><p><div class=deepzoom id=openseadragon-23 style=height:10em aria-label="Interactive zoomable viewer displaying a blue scarf with alternating bands of varied threads."></div><script>window.addEventListener("DOMContentLoaded",function(){OpenSeadragon({id:"openseadragon-23",prefixUrl:"https://cdn.jsdelivr.net/npm/openseadragon@2.4/build/openseadragon/images/",preserveViewport:true,visibilityRatio:1,minZoomLevel:1,defaultZoomLevel:1,gestureSettingsMouse:{scrollToZoom:false},tileSources:"https:\/\/iiif.princeton.edu\/loris\/iiif\/2\/figgy_prod%2F58%2F51%2Fd4%2F5851d48b225b42699a13181c778a6095%2Fintermediate_file.jp2\/info.json",});});</script><figure class=deepzoom-preview><img src=images/weaving-deepzoom-alt.jpg alt="Composite of two images from the high resolution image shown in the deep zoom viewer: one showing the full length of the woven piece, and another with a close-up showing the threads and different weaving patterns."><figcaption><p>The online version of this essay includes an interactive deep zoom viewer displaying a high resolution capture of this object.</p></figcaption></figure><div class=txt-only>⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩
| FIGURE. Interactive zoomable viewer displaying a blue scarf with alternating bands of varied threads.
⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩</div></p><h3 id=description-2>Description</h3><p>This weaving represents the references in Chapter 1 of Jacques Derrida’s <em>de la Grammatologie</em> (1967). The references have been cataloged and categorized by the research team of Derrida’s Margins.<sup id=fnref:32><a href=#fn:32 class=footnote-ref role=doc-noteref>32</a></sup> Each type of reference (epigraph, citation, quotation, footnote) is represented by a distinct yarn and weaving pattern. Derrida’s highly intertextual writing suggested the idea of weaving.<sup id=fnref:33><a href=#fn:33 class=footnote-ref role=doc-noteref>33</a></sup> Using yarn to symbolize the foundational work of deconstructionism, which operates by finding the place where a text unravels, gives additional depth to this physicalization.</p><p>Working with textiles is often stereotyped as female activity; therefore this piece also raises questions of gender and other false binaries such as art versus craft, high- versus low-tech. Based on anthropological research, women produced most of the textiles in the ancient world, but that work can be read as female authorship involved in the earliest textual practices.<sup id=fnref:34><a href=#fn:34 class=footnote-ref role=doc-noteref>34</a></sup> The loom itself runs the gamut from high to low technology: a backstrap loom can be assembled at home from dowels, rods, and cords; and yet, Joseph-Marie Jacquard’s 1801 power loom, which used punch cards to automatically create elaborate woven patterns, was an important precursor to early computers.</p><figure><img loading=lazy alt="The weaver sits in front of a table top loom; one hand lifts two strand of the warp yarn, the other stretches out the yarn being looped around it." src=/issues/1/data-beyond-vision/images/weaving-soumak.jpg sizes="(max-width: 768px) 100vw, 80vw" srcset="/issues/1/data-beyond-vision/images/weaving-soumak_hu4c20ea29d73279e64865a8a2b7a14231_9638085_500x0_resize_q75_box.jpg 500w,
/issues/1/data-beyond-vision/images/weaving-soumak_hu4c20ea29d73279e64865a8a2b7a14231_9638085_800x0_resize_q75_box.jpg 800w,
/issues/1/data-beyond-vision/images/weaving-soumak_hu4c20ea29d73279e64865a8a2b7a14231_9638085_1200x0_resize_q75_box.jpg 1200w,
/issues/1/data-beyond-vision/images/weaving-soumak_hu4c20ea29d73279e64865a8a2b7a14231_9638085_1500x0_resize_q75_box.jpg 1500w,
/issues/1/data-beyond-vision/images/weaving-soumak_hu4c20ea29d73279e64865a8a2b7a14231_9638085_1800x0_resize_q75_box.jpg 1800w,
/issues/1/data-beyond-vision/images/weaving-soumak.jpg 3400w" class=landscape><figcaption><p>Gissoo Doroudian, creating a Soumak weave.</p></figcaption></figure><div class=txt-only>⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩
| FIGURE. The weaver sits in front of a table-top loom; one hand lifts two strand of the warp yarn, the other stretches out the yarn being looped around it.
|
| CAPTION: Gissoo, creating a Soumak weave.
⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩</div><h3 id=insights-2>Insights</h3><p>The data encoded in the weaving could be represented as a stacked bar chart, a familiar and easily available choice for communicating types and quantities. However, the bar chart may be the least effective for communicating the depth and conceptual nuance of data on multiple levels.</p><figure aria-describedby=refs-desc><img loading=lazy alt="Stacked bar chart showing number and kind of references by page." style=max-height:500px src=/issues/1/data-beyond-vision/images/derrida-refsbytype-chap1.svg role=img><figcaption><p>References in Chapter 1 of <em>de la Grammatologie</em> by page and type. (Chapter 1 begins on page 15).</p></figcaption></figure><div class=txt-only>⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩
| FIGURE. Stacked bar chart showing number and kind of references by page.
|
| CAPTION: References in chapter 1 of De la grammatologie by page and type. (Chapter 1 begins on page 15).
⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩</div><div class=sr-only id=refs-desc>The chart shows that quotations are the most common type of reference in Chapter 1, and a few pages include six or more quotations. Footnotes are the next most common reference; there is only one epigraph, on the first page in the chapter, and one citation, on page 35. Only six pages have no references at all, and many pages have more than one.</div><p>The ability to feel the density and frequency of each type of reference, color coded above, creates a unique experience for each participant, specific to their own perspective. These organic experiences bring to light the depth and complexities of the original work as well as the labor involved with gathering this data. This woven piece, which represents the first half (thirteen pages) of the first chapter of <em>De la grammatologie</em> is thirty-seven inches long, a little more than three feet. The physical nature of this data representation required that materials and dimensions be carefully calculated and measured.<sup id=fnref:35><a href=#fn:35 class=footnote-ref role=doc-noteref>35</a></sup> The process of creating this piece is embodied and experiential, which naturally leads to conversations that effortlessly surface the labor of data work and the depth of the original text. Unfortunately, this is less likely to happen naturally when creating data visualizations.</p><h3 id=next-steps-2>Next Steps</h3><p>Adding conductive thread and sensors could turn the weaving into an interface, so that touching the fabric would bring up the relevant reference on an associated screen. Data weavings could also be augmented with other media, such as light and sound, to convey other aspects of the same or related data. Incorporating other work on automated weaving and knitting machines would add to the variety of options for data textiles.</p><p><a href=/issues/1/weaving-howto/>Make one ></a></p><div class=icon-nav><span class=help>Choose a different object</span>
<a href=#folding><img src=images/icon-folding.svg alt="read folding section"></a>
<a href=#modeling><img src=images/icon-printing.svg alt="read modeling section"></a>
<a href=#stacking><img src=images/icon-stacking.svg alt="read stacking section"></a></div><h2 id=stacking>Stacking New and Continuing Membership Activities of the Shakespeare and Company Lending Library</h2><p><em>Xinyi Li</em></p><figure><img loading=lazy alt="Animated GIF with the camera panning revealing different portions of the paper model." src=/issues/1/data-beyond-vision/images/stacking-horizontal-pan.gif><figcaption><p>Overview of a folded model representing the lending library membership activities from 1919 to 1941.</p></figcaption></figure><div class=txt-only>⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩
| FIGURE. Animated GIF with the camera panning revealing different portions of the paper model.
|
| CAPTION: Overview of a folded model representing the lending library membership activities from 1919 to 1941.
⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩</div><h3 id=goal-3>Goal</h3><p>This piece aims to reveal the continuity and growth of Sylvia Beach’s lending library and bookshop by showing the extent of activity and recorded membership based on logbooks and lending library cards. Multiple variables are encoded in the dimensions of stacking boxes based on the technique of pop-up box folds. By exhibiting the evolution of the library over time while contrasting activities of new and old members, this piece enables multiple ways to compare and interpret the data. By transforming a flat surface to a three-dimensional form with play of light and shadows, this production technique serves as a metaphor for the larger purpose of the <a href=https://shakespeareandco.princeton.edu/>Shakespeare and Company Project</a> — bringing archival data to life and facilitating rich interpretations.</p><figure><img loading=lazy alt="Textual labels overlaying a 3d rendering of a unit of the folded model along its three directions." src=/issues/1/data-beyond-vision/images/stacking-photo-legend.png sizes="(max-width: 768px) 100vw, 80vw" srcset="/issues/1/data-beyond-vision/images/stacking-photo-legend_hu82654c780d94fbe639e7d55a501ef41b_40394428_500x0_resize_box_2.png 500w,
/issues/1/data-beyond-vision/images/stacking-photo-legend_hu82654c780d94fbe639e7d55a501ef41b_40394428_800x0_resize_box_2.png 800w,
/issues/1/data-beyond-vision/images/stacking-photo-legend_hu82654c780d94fbe639e7d55a501ef41b_40394428_1200x0_resize_box_2.png 1200w,
/issues/1/data-beyond-vision/images/stacking-photo-legend_hu82654c780d94fbe639e7d55a501ef41b_40394428_1500x0_resize_box_2.png 1500w,
/issues/1/data-beyond-vision/images/stacking-photo-legend_hu82654c780d94fbe639e7d55a501ef41b_40394428_1800x0_resize_box_2.png 1800w,
/issues/1/data-beyond-vision/images/stacking-photo-legend.png 6980w" class=landscape><figcaption><p>Legend showing how to read the information represented in three dimensions.</p></figcaption></figure><div class=txt-only>⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩
| FIGURE. Textual labels overlaying a 3D rendering of a unit of the folded model along its three directions.
|
| CAPTION: Legend showing how to read the information represented in three dimensions.
⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩</div><h3 id=description-3>Description</h3><p>Shakespeare and Company Project lending library membership data from 1919 to 1941 are represented as a hybrid of time-series and stacked bar charts showing part-to-whole relationships made from paper and folding. Each unit, a cuboid in space and sometimes its stacking child, represents one year and displays nine variables for that year. The height corresponds to the number of active members recorded in the <a href=https://shakespeareandco.princeton.edu/sources/logbooks/>logbooks</a>; the depth depicts the number of members with borrowing activity, according to each member’s <a href=https://shakespeareandco.princeton.edu/sources/cards/>lending library card</a>; the length along the timeline is based on the total number of borrowing events.<sup id=fnref:36><a href=#fn:36 class=footnote-ref role=doc-noteref>36</a></sup> Each of the variables is split into two parts: previous members who have renewed a membership contrasted with new members. The upper portion shows the growth and the activities of new readers. Viewers can see the rise and fall of members, inspect the difference between members with borrowing activity and the members as represented in the logbooks, compare the growth over time by viewing the stacking part from the front, and survey the involvement of continuing members versus new members, to name a few possibilities. In some cases, a small number of new members were very active readers based on their borrowing activity.</p><p><div class=sketchfab-embed-wrapper><iframe id=sketchfab-96403a4659414537b470f03da96d7a88 aria-label="3D model showing a folded long paper as base, with opened cuts folded into additional panels." frameborder=0 allow="autoplay; fullscreen; vr" mozallowfullscreen=true webkitallowfullscreen=true loading=lazy src="https://sketchfab.com/models/96403a4659414537b470f03da96d7a88/embed?autospin=0.2&autostart=0&camera=0&preload=1&ui_controls=1&ui_infos=1&ui_inspector=1&ui_stop=1&ui_watermark=1&ui_watermark_link=1"></iframe><figure class=preview><img src=images/stacking-3d-alt.jpg alt="Three photos from multiple angles showing a folded long paper as base, with opened cuts folded into additional panels."><figcaption><p>The online version of this essay includes an interactive 3D viewer displaying a model of this object.</p></figcaption></figure></div><div class=txt-only>⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩
| FIGURE. 3D model showing a folded long paper as base, with opened cuts folded into additional panels.
⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩</div></p><p>This physicalization made use of kirigami technique, which transforms single sheets of paper into three-dimensional forms. Data was mapped to the shapes with <a href=http://data-illustrator.com/>Data Illustrator</a>, semi-manual calculation, and vector drawing. The materiality and the process have a poetic connection to the subject matter, metaphorically: stories and knowledge can be embedded on flat papers, but the act of reading unfolds the surface into interpretive space even beyond three dimensions.</p><h3 id=insights-3>Insights</h3><figure aria-describedby=stackedbar-desc><img loading=lazy alt="A series of stacked bar charts comparing the number of members, numbers of borrowers, and borrowing events, and the portion of new and renewed members among each of these categories." src=/issues/1/data-beyond-vision/images/2d-stacked-bar.svg role=img><figcaption><p>Membership activities of the Shakespeare and Company lending library from 1919 to 1941 represented as stacked bar charts.</p></figcaption></figure><div class=txt-only>⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩
| FIGURE. A series of stacked bar charts comparing the number of members, numbers of borrowers, and borrowing events, and the portion of new and renewed members among each of these categories.
|
| CAPTION: Membership activities of the Shakespeare and Company Lending Library from 1919 to 194 represented as stacked bar charts.
⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩</div><div class=sr-only id=stackedbar-desc>Members with borrowing events constitute a fraction of all the members in the earlier years. The overall number of borrowing events increases gradually from 1919 to 1926, drops off slightly, then evidently increases from 1934 to 1939, even though the number of members drops drastically after 1929. It’s not easy to compare book borrowing by new members across the years.</div><p>With current off-the-shelf visualization tools like the ones that come with Google Sheets, these three data series can generate three separate stacked bar charts. Since the numbers have different ranges, the vertical axes are drawn in different scales, which makes comparison across series impossible. The aggregate version presented here required manual adjustment to combine the separated charts and to make the Y axes comparable. In this 2D version, various activities of the growing membership body are not linked, and it’s difficult to draw connections between active members and the intensity of their activities because spatially these bars are not adjacent to each other. In the conventional pie charts, although the part-whole relationship between new membership activity and all activity is apparent, comparing across the three types of activities is not possible.</p><figure aria-describedby=pieseries-desc><img loading=lazy alt="A series of pie charts comparing the number of members, numbers of borrowers, borrowing events, and the portion of new and renewed members among each of these categories." src=/issues/1/data-beyond-vision/images/2d-pie.svg role=img><figcaption><p>Membership activities of the Shakespeare and Company lending library between 1919 and 1941 represented as pie charts.</p></figcaption></figure><div class=txt-only>⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩
| FIGURE. A series of pie charts comparing the number of members, numbers of borrowers, borrowing events, and the portion of new and renewed members among each of these categories.
|
| CAPTION: Membership activities of the Shakespeare and Company lending library between 1919 and 1941 represented as pie charts.
⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩</div><div class=sr-only id=pieseries-desc>Overall, renewed members outsize new members before 1929, followed by five years with nearly two times more new members than renewed members. Members with borrowing events represent a fraction of all the members in the early years between 1919 and 1929. Then members with borrowing events grow and become more comparable to the size of all members, which sizes down drastically after 1929. Borrows from new members outsize renewed members since 1934. It is not easy to compare the proportion of growth across the years.</div><p>Extending into physical space allows data to be encoded in three axes and provides multiple possible angles to view the piece, depending on the relationships one is interested in. Different angles can reveal new interactions between logbook members, members with lending library card activities, and borrowing events. The separation of renewing and new members makes it possible to juxtapose and compare activities between the two groups of members. Spatial factors communicate different facets of the data, and color coding is no longer required.</p><p>Throughout the process of developing this project as practice-based research, making and reflecting are in constant oscillation, and knowing happens in actions. As I worked through this piece and observed the artifact, new questions emerged: how do we measure the level of liveliness of the lending library? Is it by the number of members or by the number of borrowing events? Instead of drawing comparisons by the lengths along each axis, perhaps perhaps we might look at the vibrancy of the lending library in a different way: through the volumes and relative sizes of the cuboids and rectangles, which factor in both the number of members and the borrowing activity.</p><h3 id=next-steps-3>Next Steps</h3><p>The pattern generation process is very programming-friendly and the materials required are also easily accessible. The data encoding process could be automated by custom code, which could then be made available as a tool for presenting part-to-whole relationships in other datasets. With the addition of dynamic media such as projection mapping, this piece could convey more context and narratives around the lending library.</p><p><a href=/issues/1/stacking-howto/>Make one ></a></p><div class=icon-nav><span class=help>Choose a different object</span>
<a href=#folding><img src=images/icon-folding.svg alt="read folding section"></a>
<a href=#modeling><img src=images/icon-printing.svg alt="read modeling section"></a>
<a href=#weaving><img src=images/icon-weaving.svg alt="read weaving section"></a></div><h2 id=acknowledgments>Acknowledgments</h2><p>Photos of the data physicalizations on display and Gissoo Doroudian weaving are by Shelley Szwast. The high resolution capture of the data weaving used for the deep zoom and margin image was created by the Digital Imaging Studio, Princeton University Library. The photo of the CDH main space with empty tables is by Mana Winters. The custom visuals for this essay were created by Doroudian; the icons for each section were created by Doroudian and revised by Doroudian and Xinyi Li. All other photos, charts, and models were created by the authors.</p><p>Thanks to our collaborators on the <a href=https://shakespeareandco.princeton.edu/about/credits/>Shakespeare and Company Project team</a> and <a href=https://derridas-margins.princeton.edu/credits/>Derrida’s Margins project team</a> for their work to create the data we have experimented with and physicalized here; to the <a href=http://ach2019.ach.org/cfp-call-for-participation-en/>ACH2019 conference organizers for the generous CFP</a> with the option of installations, which led in part to this work; and to the <a href=https://cdh.princeton.edu/>Center for Digital Humanities at Princeton</a> and our colleagues there for the tremendous support and encouragement for this project. Special thanks to our colleagues at Princeton University Library, Meghan Testerman and Annette Jushchuk, for participating in a weaving exhibition test run and giving us feedback to improve the instructions.</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>David Staley, “On the ‘Maker Turn’ in the Humanities,” in <em>Making Things and Drawing Boundaries: Experiments in the Digital Humanities</em>, ed. Jentery Sayers (Minneapolis: University of Minnesota Press, 2017), 32–41, <a href=https://doi.org/10.5749/j.ctt1pwt6wq.5>https://doi.org/10.5749/j.ctt1pwt6wq.5</a>. <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p>“Any communicating object that reflects choices about the selection and representation of reality is a rhetorical object.” Catherine D’Ignazio and Lauren F. Klein, <em>Data Feminism</em> (Cambridge: MIT Press, 2020), 78. <a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3 role=doc-endnote><p>These pieces are based on statements—partly inspired by artist statements—that were included as part of the “Data Beyond Vision” installation presented at the ACH2019 conference. However, we have considerably expanded and adapted them, moving beyond that format. Rebecca Sutton Koeser, Nick Budak, Gissoo Doroudian, and Xinyi Li, “Data Beyond Vision” (installation, ACH2019, Pittsburgh, PA, July 25, 2019). <a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4 role=doc-endnote><p>The term “data physicalization” can be understood in three ways: as a “physical artifact whose geometry or material properties encode data”; as the process of giving physical form to data; or, as the research area combining data visualization and tangible user interfaces. See Yvonne Jansen, Pierre Dragicevic, Petra Isenberg, Jason Alexander, Abhijit Karnik, Johan Kildal, Sriram Subramanian, and Kasper Hornbæk, “Opportunities and Challenges for Data Physicalization,” in <em>CHI ’15: Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems</em> (April 2015), 3227–36, <a href=https://doi.org/10.1145/2702123.2702180>https://doi.org/10.1145/2702123.2702180</a>. <a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5 role=doc-endnote><p>To get a better sense of the variety and historic range of data physicalizations, browse a gallery of physical visualizations and related artifacts at <a href=http://dataphys.org/list/gallery/>http://dataphys.org/list/gallery/</a>. <a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6 role=doc-endnote><p>Read more about “Critical Making”, which was coined by Matt Ratto, at <a href=https://criticalmaking.com/matt-ratto/>https://criticalmaking.com/matt-ratto/</a>. <a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7 role=doc-endnote><p>This work is strongly aligned with the principles of data feminism—in particular, elevating emotion and embodiment; considering context; and making labor visible. D’Ignazio and Klein, <em>Data Feminism</em>, 17–18. <a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8 role=doc-endnote><p>Donna Haraway, &ldquo;Situated Knowledges: The Science Question in Feminism and the Privilege of Partial Perspective,&rdquo; <em>Feminist Studies</em> 14, no. 3 (Autumn 1988): 581. <a href=https://doi.org/10.2307/3178066>https://doi.org/10.2307/3178066</a>. <a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9 role=doc-endnote><p>D’Ignazio and Klein, <em>Data Feminism</em>, 76. <a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:10 role=doc-endnote><p>We’re trying out this approach on research partnership projects we’ve worked on at the Center for Digital Humanities at Princeton. Read the <a href=https://github.com/princeton-cdh/mep-django/issues/404>GitHub issue where we discussed the implementation</a>, the <a href=https://www.sarasoueidan.com/blog/accessible-data-charts-for-khan-academy-2018-annual-report/>article that inspired our approach</a>, and an example of it in use in a <a href=https://prosody.princeton.edu/editorial/2020/01/visualizing-collections/>Princeton Prosody Archive Editorial essay</a>. The journal you are now reading also experiments with this approach. See the <a href=https://github.com/Princeton-CDH/startwords/pull/95>GitHub pull request</a> where we discussed a plain text caption and alt-text schema. <a href=#fnref:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:11 role=doc-endnote><p>The difficulty and unfamiliarity of this intimacy may be demonstrated in part by our experience with displaying these objects. We often have to encourage and reassure people that they are allowed to touch these items, even though there are signs clearly posted inviting just that. <a href=#fnref:11 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:12 role=doc-endnote><p>&ldquo;Mastery&rdquo; is a problematic term, as African Americans and members of other marginalized groups know well, and as the racial unrest in the United States this summer has made White Americans more aware. Klein and D’Ignazio also point out that the term has a gendered component as well, with the master stereotype associated with men across many Western cultures. <em>Data Feminism,</em> 77. For a discussion of “master” and its unfortunate use in technology, see “Toward anti-racist technical terminology,” Association for Computers and the Humanities (ACH), accessed October 22, 2020, <a href=https://ach.org/toward-anti-racist-technical-terminology/>https://ach.org/toward-anti-racist-technical-terminology/</a> <a href=#fnref:12 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:13 role=doc-endnote><p>“Levinas is one of few philosophers to displace the metaphor of vision dominating ontological accounts of intersubjectivity into a metaphor of touch … which resonates with Irigaray’s own reformulation of subject-object relations in the figure of the two lips.” Ince, Kate Ince, “Questions to Luce Irigaray,” <em>Hypatia</em> 11, no. 2 (1996): 122–40, <a href=http://www.jstor.org/stable/3810267>http://www.jstor.org/stable/3810267</a> <a href=#fnref:13 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:14 role=doc-endnote><p>“This gesture . . . which weds without consum(mat)ing .. may be called: the touch of the caress. . . . This touch binds and unbinds two others in a flesh that is still and always untouched by mastery.” Luce Irigaray, <em>An Ethics of Sexual Difference</em>, trans. Carolyn Burke and Gillian C. Gill (Ithaca: Cornell Press, 1993), 186. <a href=#fnref:14 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:15 role=doc-endnote><p>This technique, also called “modular origami,” uses separate sheets folded into repeated interlocking small forms. The practice is attested as early as the eighteenth century, but gained popularity in the 1960s with the work of Robert Neale in the United States and Mitsunobu Sonobe in Japan. Our model uses forms from Tomoko Fuse’s book, <em>Unit Origami: Multidimensional Transformations</em> (Tokyo: Japan Publications, 1990). <a href=#fnref:15 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:16 role=doc-endnote><p>VIAF aggregates records from multiple national libraries with authoritative names for people, organizations, and titles. Typically only people associated with published works, such as authors and translators, have records in VIAF; we used this as a proxy for some degree of fame, even though there are plenty of names in VIAF that are not strictly famous. <a href=#fnref:16 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:17 role=doc-endnote><p>This work is based on pre-release versions of the datasets now published as: Joshua Kotin, Rebecca Sutton Koeser, Carl Adair, Serena Alagappan, Jean Bauer, Oliver J. Browne, Nick Budak, Harriet Calver, Jin Chow, Ian Davis, Gissoo Doroudian, Currie Engel, Elspeth Green, Benjamin Hicks, Madeleine E. Joelson, Carolyn Kelly, Sara Krolewski, Xinyi Li, Ellie Maag, Cate Mahoney, Jesse D. McCarthy, Mary Naydan, Isabel Ruehl, Sylvie Thode, Camey VanSant, and Clifford E. Wulfman, <em>Shakespeare and Company Project Dataset: Lending Library Members, Books, Events</em>, version 1.0, July 2020, distributed by DataSpace, Princeton University, <a href=https://doi.org/10.34770/pe9w-x904>https://doi.org/10.34770/pe9w-x904</a>. For more information, see <a href=https://shakespeareandco.princeton.edu/about/data/>https://shakespeareandco.princeton.edu/about/data/</a> <a href=#fnref:17 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:18 role=doc-endnote><p>This is inspired in part by ethical principles from the Colored Conventions Project, which asks researchers using their data to “contextualize and narrate the conditions of the people who appear as ‘data’ and to name them when possible.” “Introduction to CCP Corpus,” Colored Conventions Project, accessed October 22, 2020, <a href=https://coloredconventions.org/about-records/ccp-corpus/>https://coloredconventions.org/about-records/ccp-corpus/</a>. <a href=#fnref:18 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:19 role=doc-endnote><p>We wanted to include a 3D model of this physicalization in order to provide some semblance of virtually grasping and rotating the object. Because we didn’t have a way to capture the actual object in 3D, we created a model of it in <a href=https://www.blender.org/>Blender</a>; this was fairly straightforward, since it consists of two simple shapes. <a href=#fnref:19 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:20 role=doc-endnote><p>For more on the strengths and weaknesses of the pie chart, see Steve Johnson, “The Case Against Pie Charts,” University of Utah Health, March 3, 2017, <a href=https://accelerate.uofuhealth.utah.edu/connect/steves-dojo-7-the-case-against-pie-charts>https://accelerate.uofuhealth.utah.edu/connect/steves-dojo-7-the-case-against-pie-charts</a>. <a href=#fnref:20 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:21 role=doc-endnote><p>We do not provide a how-to for this object out of respect for the copyright of origami authors, who often spend years developing and refining single designs. The three-dimensional forms used to make this piece are a combination of two modular origami units which are the original creations of the brilliant Tomoko Fuse, and can be found in her book <em>Unit Origami</em>. The octahedron is formed from eight units with triangular windows, and these windows are filled with eight so-called E-b units that create the illusion of an intersecting cube. Fuse, <em>Multidimensional Origami,</em> 109, 233. When assembling from your own paper, you can print <a href=https://startwords.cdh.princeton.edu/issues/1/data-beyond-vision/non_viaf_names.pdf>a document containing the names of the non-VIAF members of the library</a> onto the paper before folding the octahedron. Sheets of 8.5 inch x 11 inch copy paper can become difficult to manipulate when folded many times; thinner paper is easier to work with. <a href=#fnref:21 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:22 role=doc-endnote><p>A <a href=https://datavizproject.com/data-type/lollipop-chart/>lollipop chart</a> is a bar chart variant that uses dots to mark the values. Earlier prototypes of this model were based on bar charts, but I discovered that the space between bars in a lollipop chart made it easier to “read” the 3D version, since the thinner vertical supports obscure less of the model visually. <a href=#fnref:22 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:23 role=doc-endnote><p>This work is based on pre-release versions of the <a href=https://shakespeareandco.princeton.edu/about/data/>datasets now available</a>. Kotin et al., <em>Shakespeare and Company Project Dataset: Lending Library Members, Books, Events.</em> <a href=#fnref:23 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:24 role=doc-endnote><p>We use white and green in our physicalizations of data from the Shakespeare and Company Project because green is the most iconic color used in the site design. <img src=images/scp_colors_dark.png alt="Color palette from Shakespeare and company project."> <a href=#fnref:24 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:25 role=doc-endnote><p>To better understand the Shakespeare and Company Project sources, see Shakespeare and Company Project, “Lending Library Cards,” Center for Digital Humanities, Princeton University, November 20, 2019, <a href=https://shakespeareandco.princeton.edu/sources/cards/>https://shakespeareandco.princeton.edu/sources/cards/</a>; and Shakespeare and Company Project, “Logbooks,” Center for Digital Humanities, Princeton University, November 20, 2019, <a href=https://shakespeareandco.princeton.edu/sources/logbooks/>https://shakespeareandco.princeton.edu/sources/logbooks/</a>. To understand how the Project data from those sources fit together, read Joshua Kotin and Rebecca Sutton Koeser, “The Shakespeare and Company Lending Library Cards in Context,” Shakespeare and Company Project, Center for Digital Humanities, Princeton University. March 9, 2020, <a href=http://shakespeareandco.princeton.edu/analysis/2020/03/shakespeare-and-company-lending-library-cards-context/>http://shakespeareandco.princeton.edu/analysis/2020/03/shakespeare-and-company-lending-library-cards-context/</a>. <a href=#fnref:25 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:26 role=doc-endnote><p>The previous version was more complicated to print; the overlapping framework interfered with small data points, and the tightly interlocking parts did not scale well. <img src=images/modeling-ach2019-side-view.jpg alt="Side view of early model version draft for ACH 2019 conference."> <img src=images/modeling-ach2019-top-view.jpg alt="Top view of early model version draft for ACH 2019 conference."> <a href=#fnref:26 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:27 role=doc-endnote><p>Due to COVID-19, I’m writing and revising this from my home office without physical access to the object in question. I find myself closing my eyes and extending my fingers to focus on my memories of touching the object. <a href=#fnref:27 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:28 role=doc-endnote><p>For an exploration of the subjectivity, uncertainty, and interpretation implicit in data, read <a href=https://startwords.cdh.princeton.edu/issues/1/their-data-ourselves/>Rebecca Munson’s essay</a> in this issue of <em>Startwords</em>. For an overview of common techniques for representing uncertainty visually, see Nathan Yau, “Visualizing the Uncertainty in Data,” <em>FlowingData</em> (blog), January 8, 2018, <a href=https://flowingdata.com/2018/01/08/visualizing-the-uncertainty-in-data/>https://flowingdata.com/2018/01/08/visualizing-the-uncertainty-in-data/</a>. For more on the difficulty people have in recognizing uncertainty in data visualization, see D’Ignazio and Klein on “Visceralizing Uncertainty” in <em>Data Feminism</em>, 88. <a href=#fnref:28 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:29 role=doc-endnote><p>These have not yet been tested by anyone who reads braille. <a href=#fnref:29 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:30 role=doc-endnote><p><a href=https://www.blender.org/>Blender</a> is an open-source 3D modeling creation suite which includes a Python API. I experimented with multiple other solutions for generating 3D models programmatically, including <a href=https://www.openscad.org/>OpenSCAD</a> and <a href=https://github.com/dbrgn/tangible>Tangible</a>, but found they were too limited and couldn’t handle a model of the size and complexity I needed to generate for this dataset, so I eventually settled on Blender and Python. The Blender interface makes it possible to view and modify models generated from code, there is <a href=https://docs.blender.org/api/current/>documentation for the Python API</a>, and I found many helpful answers on <a href=https://blender.stackexchange.com/>Blender Stack Exchange</a>. <a href=#fnref:30 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:31 role=doc-endnote><p>The word “text” comes from Latin <em>textus,</em> literally “thing woven” and from the verb <em>texere,</em> “to weave.” According to Kathryn Sullivan Kruger, “the connection between weaving (textiles) and language (texts) becomes so entangled as to be almost impossible to separate. In many languages, including English, the verb to weave defines not just the making of textiles, but any creative act.” Kathryn Sullivan Kruger, <em>Weaving the Word: The Metaphorics of Weaving and Female Textual Production</em> (Cranbury, NJ: Rosemont Publishing & Printing, 2001), 29. To play on one of Derrida’s most famous statements, “<em>il n&rsquo;y a pas de hors-textile.</em>” <a href=#fnref:31 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:32 role=doc-endnote><p>See the <em>Derrida&rsquo;s Margins</em> project at <a href=https://derridas-margins.princeton.edu/>https://derridas-margins.princeton.edu/</a>. Kate Chenoweth et al., “References in Jacques Derrida&rsquo;s <em>de la Grammatologie</em>,” September 10, 2018, Figshare, <a href=https://doi.org/10.6084/m9.figshare.7180448.v1>https://doi.org/10.6084/m9.figshare.7180448.v1</a>. <a href=#fnref:32 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:33 role=doc-endnote><p>For a deeper exploration of textile metaphors in Derrida’s work, see Caroline Rooney’s “Deconstruction and Weaving” in <em>Deconstructions: A User&rsquo;s Guide,</em> ed. Nicholas Royle (London: Palgrave, 2000), <a href=https://doi.org/10.1007/978-1-137-06095-2_14>https://doi.org/10.1007/978-1-137-06095-2_14</a>. <a href=#fnref:33 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:34 role=doc-endnote><p>See Kruger, <em>Weaving the Word,</em> 21, 34, 136. <a href=#fnref:34 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:35 role=doc-endnote><p>When we first did the math on the weaving and measured, we discovered that doing all of Chapter 1 would require a warp longer than three tables end-to-end in the CDH main space, which is why we scaled back to just half of the chapter. <img src=images/cdh-empty-tables.jpg alt="read of empty tables in main space at CDH."> <a href=#fnref:35 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:36 role=doc-endnote><p>This work is based on pre-release versions of the <a href=https://shakespeareandco.princeton.edu/about/data/>datasets now available</a>. Kotin et al., <em>Shakespeare and Company Project Dataset: Lending Library Members, Books, Events.</em> <a href=#fnref:36 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></div></div></article></main><footer><nav aria-label="footer links"><ul><li><a class=highlight-focus href=/about/>About</a></li></ul></nav><div class=icons><a class="license highlight-focus" rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons Attribution 4.0 International License" src=/img/logos/license.svg></a>
<a class=highlight-focus href=/ aria-label=Startwords><div class=logo id=startwords></div></a><a class=highlight-focus href=https://cdh.princeton.edu/ aria-label="The Center for Digital Humanities at Princeton"><div class=logo id=cdh></div></a></div></footer></body></html>