<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#ff0000"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=apple-touch-icon sizes=180x180 href=/img/favicon/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/img/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/img/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=384x384 href=/img/favicon/android-chrome-384x384.png><link rel=icon type=image/png sizes=192x192 href=/img/favicon/android-chrome-192x192.png><link rel=icon type=image/png sizes=150x150 href=/img/favicon/mstile-150x150.png><link rel="shortcut icon" href=/favicon.ico><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/img/favicon/safari-pinned-tab.svg color=#ce2949><link rel=preload as=font href=/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-regular.woff2><link rel=preload as=font href=/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-italic.woff2><link rel=preload as=font href=/fonts/Source_Sans_Pro/source-sans-pro-v13-latin-regular.woff2><link rel=preload as=font href=/fonts/Source_Sans_Pro/source-sans-pro-v13-latin-italic.woff2><link rel=preconnect href=https://cdn.jsdelivr.net><link rel=preconnect href=https://static.sketchfab.com><link rel=preconnect href=https://media.sketchfab.com><title>Data Beyond Vision</title><meta name=description content="Startwords Issue 1, October 2020. A research periodical irregularly published by the Center for Digital Humanities at Princeton."><meta property="og:title" content="Data Beyond Vision"><meta property="og:description" content="How do we represent tangible objects in a visual medium? We use words, pictures, and diagrams. We describe, share, show, and fail."><meta property="og:type" content="article"><meta property="og:url" content="https://startwords.cdh.princeton.edu/issues/1/data-beyond-vision/"><meta property="og:image" content="https://startwords.cdh.princeton.edu/issues/1/data-beyond-vision/images/dbv-social.jpg"><meta property="article:published_time" content="2020-10-01T00:00:00+00:00"><meta property="article:modified_time" content="2020-10-14T17:47:38-04:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://startwords.cdh.princeton.edu/issues/1/data-beyond-vision/images/dbv-social.jpg"><meta name=twitter:title content="Data Beyond Vision"><meta name=twitter:description content="How do we represent tangible objects in a visual medium? We use words, pictures, and diagrams. We describe, share, show, and fail."><link rel=stylesheet href=/style.css><link rel=stylesheet href=/print.css media=print><link rel=stylesheet href=/issues/1/data-beyond-vision/style.css><script defer src=https://cdn.jsdelivr.net/npm/openseadragon@2.4.2/build/openseadragon/openseadragon.min.js integrity="sha256-NMxPj6Qf1CWCzNQfKoFU8Jx18ToY4OWgnUO1cJWTWuw=" crossorigin=anonymous></script><script async type=text/javascript src=/js/vendor.js></script><script async type=text/javascript src=/js/bundle.min.js></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-87887700-8','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><link rel=alternate type=text/plain href=https://startwords.cdh.princeton.edu/issues/1/data-beyond-vision/index.txt><link rel=schema.dc href=http://purl.org/DC/elements/1.0/><meta name=citation_public_url content="https://startwords.cdh.princeton.edu/issues/1/data-beyond-vision/"><meta name=citation_title content="Data Beyond Vision"><meta name=citation_date content="2020/10"><meta name=citation_author content="Rebecca Sutton Koeser"><meta name=citation_author content="Gissoo Doroudian"><meta name=citation_author content="Nick Budak"><meta name=citation_author content="Xinyi Li"><meta name=citation_pdf_url content="https://cdh.princeton.edu/media/uploads/documents/grad_fellows_call_for_spring_21.pdf"><meta name=citation_doi content="10.5281/zenodo.3713671"><meta name=citation_abstract content="How do we represent tangible objects in a visual medium? We use words, pictures, and diagrams. We describe, share, show, and fail."><meta name=citation_journal_title content="Startwords"><meta name=citation_issn content="2694-2658"><meta name=citation_issue content="1"><meta name=citation_publisher content="Center for Digital Humanities, Princeton University"><meta name=DC.rights content="http://creativecommons.org/licenses/by/4.0/"><meta name=author content="Rebecca Sutton Koeser, Gissoo Doroudian, Nick Budak, Xinyi Li"><meta name=generator content="Center for Digital Humanities, Princeton University"><meta name=dcterms.created content="2020-10"></head><body class=article><header><nav aria-label=main><ul><li class=home><a href=/><img class=logo src=/img/logos/startwords.svg alt=Home></a></li><li class=issues><a href=/issues><span>Issues</span></a></li></ul></nav></header><main><article><div class=grid><header><p class=number><a href=/issues/1/>Issue 1</a></p><p class=theme>Transformations</p><h1>Data Beyond Vision</h1><ul class=authors><li><address>Rebecca Sutton Koeser</address></li><li><address>Gissoo Doroudian</address></li><li><address>Nick Budak</address></li><li><address>Xinyi Li</address></li></ul><br><time class=pubdate datetime=2020-10>October 2020</time><br><a href=http://doi.org/10.5281/zenodo.3713671 rel=alternate class=doi>10.5281/zenodo.3713671</a><br><p class=formats><a href=/issues/1/data-beyond-vision/index.txt rel=alternate type=text/plain>TXT</a>
<a href=https://cdh.princeton.edu/media/uploads/documents/grad_fellows_call_for_spring_21.pdf rel=alternate type=application/pdf>PDF</a></p></header><div class=text-container><p>How do we represent tangible objects in a visual medium? We use words, pictures, and diagrams. We describe, share, show, and fail.</p><p>Humanists continue to expand the range of texts they study, but the range of scholarly outputs has not seen a similar expansion. While there are movements within Digital Humanities to consider formats beyond the traditional, the presentation and publishing of more innovative and experimental works (such as installations and project demos) is still secondary or sidelined, where it exists at all. What would it look like to consider non-textual research outputs as first-order scholarly work? The historian David Staley suggests the terms “interpretive objects” or “humanistic objects” for creative scholarly acts that are not limited to text.<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> Innovative work like this is carefully researched and theorized, and deserves scholarly engagement and intellectual rigor even if it does not fit into established modes of scholarly communication.</p><p>Academic research has a long history of textual scholarly practice and citation that we haven’t yet figured out how to adapt to non-textual scholarship. The <a href=https://openhumanitiesdata.metajnl.com/>Journal of Open Humanities Data</a> and <a href=https://joss.theoj.org/>The Journal of Open Source Software</a> can
both be seen as steps in this direction: they provide venues for the review and publication of data and software, respectively, accompanied by brief “metapapers”; but even these rely on transforming the content they review (data and software) into text in order to function! What are the implications if we truly expand the range of accepted scholarly outputs to include such interpretive objects as data structures, databases, software, datasets, physical objects, augmented reality experiences? Will all scholars need to become experts in all these modes, or can we find a way to be conversant, as with other important scholarly theories?</p><div id=interlude><div class=left><p><strong>SEE</strong></p><p>from a distance.<br>Cold, commanding.<br>Sense of mastery,<br>but optical illusions deceive.</p><p>Look in a mirror.<br>and see yourself<br>seeing.</p></div><div class=right><p><strong>TOUCH</strong></p><p>up close.<br>Intimate, incomplete.<br>Explore partial knowledge,<br>enlighten slowly.</p><p>Run fingers across skin<br>and touch yourself<br>touching.</p></div></div><p>These are not artist statements because this is not art; Data Art is something else. This is representation, correspondence, laborious translation. Call them maker statements, perhaps. These are our attempts to communicate our goals and help you read and interpret these unfamiliar objects and be challenged by the potential of physicalizations.</p><p>Data physicalization focuses on constructing data in physical form. It may be similar to other approaches in that it helps to understand and represent data, and in its use of the senses to communicate information, mainly through touch and sight. However, there are considerable distinctions among these approaches. What makes data physicalization distinct is that it encourages critical making by bridging the gap between creative physical and conceptual exploration. This matters because not only does it surfaces the amount of labor involved with data production and representation, and put it into different perspectives and dimensions; it can also creates an opportunity for viewers to become participants by taking part in the making of a piece using data.</p><p>Data Physicalization attempts to defamiliarize our eyes from many of the two-dimensional data representations we have seen, and put us “in the middle of data”. There is something unique about turning data points into physical forms and placing them in space that triggers the mind to understand data in a distinctive way.<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></p><p><figure><img src=/issues/1/data-beyond-vision/images/conceptmap.svg role=img loading=lazy alt="Concept map situating data physicalization in relation to other types of data representations and interpretations."><figcaption><p>Concept map situating data physicalization in relation to other types of data representations and interpretations.
<a href=https://doi.org/10.5281/zenodo.3261531>From the “Data Beyond Vision” poster presented at DH2019, Utrecht, July 2019.</a></p></figcaption></figure><div class=txt-only>⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩
| FIGURE.
|
| CAPTION: Concept map situating data physicalization in relation to other types of data representations and interpretations.
| ATTRIBUTION: From the “Data Beyond Vision” poster presented at DH2019, Utrecht, July 2019.
| LINK: <a href=https://doi.org/10.5281/zenodo.3261531>https://doi.org/10.5281/zenodo.3261531</a>
⩩&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;⟩</div><div class=sr-only id=conceptmap-desc>Other approaches for data representation and interpretation include:</p><ul><li>Data Visualization, which focuses on storytelling by using graphical elements</li><li>Data Edibilization, which focuses on experiencing data through food using edible materials</li><li>Data Sonification, which focuses on auditory patterns by using sound</li><li>Data Visceralization, which focuses on multiple sensory experiences by using multiple senses, making it the only approach that can incorporate all of the senses</li><li>Data Art, which focuses on representing links between data and artistic creations by using expressive frameworks and raw data.</li><li>Interpretive object, which focuses on revealing meanings and relationships via non-textual forms by using metaphors.</li></ul></div></p><p>Making use of other senses can provide a step towards greater accessibility. The typical solution for making data visualization accessible to vision-impaired readers is to provide a table with the data underlying the chart or graph. This isn’t practical for large datasets, and it’s clearly not the same experience, or otherwise we would provide the tabular data to all users. Another approach is to provide an extended description of the insights gained from the chart. This is helpful, but pre-digesting the chart in this way doesn’t allow readers to view and interpret the patterns and draw their own conclusions.</p><p>We invite you to participate in the labor of data work. Download models and instructions, use your hands to recreate the data physicalizations we developed, or use these as inspiration to make your own interpretive objects.</p><div class=icon-nav><span class=help>Choose an object</span>
<a href=#folding><img src=images/icon-folding.svg alt="view folding section"></a>
<a href=#modeling><img src=images/icon-printing.svg alt="view modeling section"></a>
<a href=#weaving><img src=images/icon-weaving.svg alt="view weaving section"></a>
<a href=#stacking><img src=images/icon-stacking.svg alt="view stacking section"></a></div><h2 id=folding>Folding in the Non-Famous Members of the Shakespeare and Company Library</h2><div class=sketchfab-embed-wrapper><iframe id=sketchfab-9c96fadd27c34a11902f0a1281ea0ab4 title="Shakespeare and Company membership origami on Sketchfab" frameborder=0 allow="autoplay; fullscreen; vr" mozallowfullscreen=true webkitallowfullscreen=true loading=lazy src="https://sketchfab.com/models/9c96fadd27c34a11902f0a1281ea0ab4/embed?autospin=0.2&autostart=0&camera=0&preload=1&ui_controls=1&ui_infos=1&ui_inspector=1&ui_stop=1&ui_watermark=1&ui_watermark_link=1"></iframe></div><h3 id=goal>Goal</h3><p>The Shakespeare and Company library is most often known by its famous members — the writers of the Lost Generation and their friends. We wanted to highlight the activity of the relatively unknown members — frequently women - who in fact represent a much larger portion of the library&rsquo;s day-to-day activity and thus arguably better represent it. The piece makes use of unit origami to create a larger, cohesive form from small folded units, mirroring the relationship between a single member and the greater bulk of the library.</p><h3 id=description>Description</h3><p>The physicalization contrasts the activity of the well-known members of the library (those linked by researchers to an entry in VIAF) with the activity of its relatively unknown members (no known authority records). Activity is represented by the total number of recorded events that would plausibly have brought members into the library - checking out, renewing, and returning books. By holding the physicalization in two different ways, the user can &ldquo;grasp&rdquo; two separate sets of data: the octahedron (non-famous members) and the cube (famous members). The ratio of the volumes of these two solids is equal to the ratio of the two datasets.</p><h3 id=insights>Insights</h3><p>A pie chart can be used to present the same ratio of data conveyed in the physicalization. &mldr;.</p><h3 id=next-steps>Next Steps</h3><p>Using cut, punched, or embossed paper would make the piece more tactile; instead of simply printing names, unique patterns could be added to represent data for individual members. In the future, we could generate unique folding patterns for individual library member activity and make them available via print-on-demand, which would enable viewers to become participants and turn folding into an act of recovery of the unknown library members.</p><p>Nick Budak, Xinyi Li</p><div class=icon-nav><span class=help>Choose an object</span>
<a href=#folding><img src=images/icon-folding.svg alt="view folding section"></a>
<a href=#modeling><img src=images/icon-printing.svg alt="view modeling section"></a>
<a href=#weaving><img src=images/icon-weaving.svg alt="view weaving section"></a>
<a href=#stacking><img src=images/icon-stacking.svg alt="view stacking section"></a></div><h2 id=modeling>Modeling Shakespeare and Company Library Membership</h2><div class=sketchfab-embed-wrapper><iframe id=sketchfab-89985d66f7244d87b7edbe5fd6266f0d title="Shakespeare and Company membership lollipop chart on Sketchfab" frameborder=0 allow="autoplay; fullscreen; vr" mozallowfullscreen=true webkitallowfullscreen=true loading=lazy src="https://sketchfab.com/models/89985d66f7244d87b7edbe5fd6266f0d/embed?autospin=0.2&autostart=0&camera=0&preload=1&ui_controls=1&ui_infos=1&ui_inspector=1&ui_stop=1&ui_watermark=1&ui_watermark_link=1"></iframe></div><h3 id=goal-1>Goal</h3><p>This data physicalization demonstrates the affordances of three dimensions for representing data: time series data are displayed with sequential months and years adjacent to each other, which makes it easier to discern seasonal and annual trends. I hope to inspire others to try experimental approaches to representing data; writing software to generate printable 3D models directly from the data makes the process reproducible, and may eventually enable others to create and print their own physicalizations. The tactile nature of the object suggests the possibilities of 3D printing to create more accessible representations of data.</p><h3 id=description-1>Description</h3><p>This is a two variable, three dimensional lollipop chart showing the membership of the Shakespeare and Company library by month and year, from November 1919 when Sylvia Beach opened her bookstore to its official closing in 1942. Membership data is drawn from two different sources, both of which are incomplete: broad subscription information comes from <a href=https://shakespeareandco.princeton.edu/sources/logbooks/>logbooks</a> (although logbooks for 1930 and parts of 1931-32 are missing); detailed borrowing histories come from <a href=https://shakespeareandco.princeton.edu/sources/cards/>lending library cards</a> for a subset of members. The white represents the number of members with an active subscription in each month; the green<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> corresponds to the number of members with borrowing activity in each month. For any month where the value is zero, there is no lollipop. Representing the two different datasets as adjacent half lollipops exposes the discrepancies between the stories these sources tell us about the membership of the library without privileging either of them. The two lollipop charts are designed to be printed independently and then assembled, so that any 3D printer can be used; however, this is a prototype and the design needs further revision.</p><h3 id=insights-1>Insights</h3><p>The same data can be presented in a two-variable bar chart. Overall trends are easy to see, but seasonal trends are not as distinct. Changing perspective on the physical object allows focusing on yearly or monthly trends. Missing data in one variable is visible in both &mldr; [TBD]<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup></p><h3 id=next-steps-1>Next Steps</h3><p>The current version uses different shapes for the two variables, but adding textures would make the model even more tactile. Simple 3D labels with text and braille have been added for display alongside the piece, but perhaps they could incorporated directly on the model, and refined to provide a scale for the axes. The 3D printed objects could also be augmented with other media: lights or sound could convey intensity of borrowing activity, or threads connecting months to represent the number of subscription renewals and convey a sense of continuity. The Python code used to create these models could be generalized for reuse, and eventually made available as a Blender plugin.</p><p>Rebecca Sutton Koeser</p><div class=icon-nav><span class=help>Choose an object</span>
<a href=#folding><img src=images/icon-folding.svg alt="view folding section"></a>
<a href=#modeling><img src=images/icon-printing.svg alt="view modeling section"></a>
<a href=#weaving><img src=images/icon-weaving.svg alt="view weaving section"></a>
<a href=#stacking><img src=images/icon-stacking.svg alt="view stacking section"></a></div><h2 id=weaving>Weaving Derrida’s references</h2><blockquote><p>… we all of us, grave or light, get our thoughts entangled in metaphors…
<cite>George Eliot, Middlemarch</cite></p></blockquote><h3 id=goal-2>Goal</h3><p>With this piece, we aim to literalize the metaphor of weaving as writing by representing Derrida’s intertextuality as a woven tapestry. The textures of the yarn and woven fabric invite touch, but by showing an in-progress weaving with the pattern and instructions provided, we move viewers beyond seeing and touching to enable them to become participants in reconstructing the data. Showing the weaving in progress also foregrounds the labor of data work, since curation, collection, and visualization all take an enormous amount of work and skill, often from a range of different individuals.</p><figure><img src=/issues/1/data-beyond-vision/images/weaving_photo_hu922073d4c6e051c7a1adf75197e5e77d_1859719_1500x0_resize_q75_box.jpg role=img sizes="(max-width: 768px) 100%, 80%" loading=lazy srcset="/issues/1/data-beyond-vision/images/weaving_photo_hu922073d4c6e051c7a1adf75197e5e77d_1859719_500x0_resize_q75_box.jpg 500w,
/issues/1/data-beyond-vision/images/weaving_photo_hu922073d4c6e051c7a1adf75197e5e77d_1859719_800x0_resize_q75_box.jpg 800w,
/issues/1/data-beyond-vision/images/weaving_photo_hu922073d4c6e051c7a1adf75197e5e77d_1859719_1200x0_resize_q75_box.jpg 1200w,
/issues/1/data-beyond-vision/images/weaving_photo_hu922073d4c6e051c7a1adf75197e5e77d_1859719_1500x0_resize_q75_box.jpg 1500w,
/issues/1/data-beyond-vision/images/weaving_photo_hu922073d4c6e051c7a1adf75197e5e77d_1859719_1800x0_resize_q75_box.jpg 1800w," class=portrait alt="Setting up the warp on the loom to start weaving."><figcaption><p>Setting up the warp on the loom to start weaving.</p></figcaption></figure><div class=txt-only>Photo. [TBD. Whatever level of description (if any!) we want here.]</div><h3 id=description-2>Description</h3><p>This weaving represents the references in chapter one of Jacques Derrida’s de la Grammatologie. The references have been cataloged and categorized by researchers for Derrida’s Margins. Each type of reference (epigraph, citation, quotation, footnote) is represented by a distinct yarn and weaving pattern. Derrida’s highly intertextual writing suggested the idea of weaving, and using yarn to symbolize the foundational work of deconstructionism, which operates by finding the place where a text unravels, gives additional depth to this physicalization. Textile work is often stereotyped as women’s work, so this piece also raises questions of gender, art versus craft, and high tech versus low tech.</p><div class=deepzoom id=openseadragon-11 style=height:10em></div><script>window.addEventListener("DOMContentLoaded",function(){OpenSeadragon({id:"openseadragon-11",prefixUrl:"https://cdn.jsdelivr.net/npm/openseadragon@2.4/build/openseadragon/images/",preserveViewport:true,visibilityRatio:1,minZoomLevel:1,defaultZoomLevel:1,gestureSettingsMouse:{scrollToZoom:false},tileSources:"https:\/\/iiif.princeton.edu\/loris\/iiif\/2\/figgy_prod%2F58%2F51%2Fd4%2F5851d48b225b42699a13181c778a6095%2Fintermediate_file.jp2\/info.json",});});</script><h3 id=insights-2>Insights</h3><p>[tbd]</p><h3 id=next-steps-2>Next Steps</h3><p>Adding conductive thread and sensors could turn the weaving into an interface, so that touching the fabric would bring up the relevant reference on an associated screen. Data weavings could also be augmented with other media, such as lights and sound to convey other aspects of the same or related data. Incorporating other work on automated weaving and knitting machines would add to the variety of options for data textiles.</p><p>Rebecca Sutton Koeser, Gissoo Doroudian</p><div class=icon-nav><span class=help>Choose an object</span>
<a href=#folding><img src=images/icon-folding.svg alt="view folding section"></a>
<a href=#modeling><img src=images/icon-printing.svg alt="view modeling section"></a>
<a href=#weaving><img src=images/icon-weaving.svg alt="view weaving section"></a>
<a href=#stacking><img src=images/icon-stacking.svg alt="view stacking section"></a></div><h2 id=stacking>Stacking New and Continuing Membership Activities of Shakespeare and Company Library</h2><h3 id=goal-3>Goal</h3><p>This piece aims to reveal the continuity and growth of Sylvia Beach’s lending library by showing the extent of activity and recorded membership based on logbooks and lending cards. Multiple variables are encoded in the dimensions of stacking boxes based on the technique of pop-up box folds. By exhibiting the evolution of the library over time while contrasting activities of new and old patrons, this piece enables multiple ways to compare and interpret the data from diverse perspectives. By transforming a flat surface to a three-dimensional form with play of light and shadows, this production technique serves as a metaphor for the purpose of larger Shakespeare and Company Project— bringing archival data to life and facilitating rich interpretations<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup></p><h3 id=description-3>Description</h3><p>Shakespeare and Company Project library membership data from 1919 to 1941 are represented as a hybrid of time-series and stacked bar charts showing part-to-whole relationship made from paper and folding. Each unit, a cuboid in space and sometimes its stacking child, represents one year and displays nine variables for that year. The height corresponds to the number of active subscribers recorded in the logbooks; the depth depicts the number of members with borrowing activity, according to each member’s lending library card; the length along the timeline is based on the total number of borrowing events. Each of the variables is split into two parts: previous patrons who have renewed a subscription contrasted with new members. The upper portion shows the growth and the activities of new readers. Viewers can see the rise and fall of subscribers, inspect the difference between active borrowers and subscribers, compare the growth over time by viewing the stacking part from the front, and survey the involvement of continuing members versus new patrons, to name a few possibilities. In some cases, a small number of new members were very active readers based on their borrowing activity. Data was mapped to the shapes with Data Illustrator, semi-manual calculation and vector drawing.</p><h3 id=insights-3>Insights</h3><p>[tbd]</p><h3 id=next-steps-3>Next Steps</h3><p>The data encoding process could be automated by writing custom code, which could then be made available as a tool for presenting part-to-whole relationships. With the addition of time-based media such as projection mapping, this piece could convey more context and narratives around the lending library.</p><p>Xinyi Li</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>Staley, David. “On the ‘Maker Turn’ in the Humanities.” In <em>Making Things and Drawing Boundaries</em>, edited by Jentery Sayers, 32–41. Experiments in the Digital Humanities. University of Minnesota Press, 2017. <a href=https://doi.org/10.5749/j.ctt1pwt6wq.5>https://doi.org/10.5749/j.ctt1pwt6wq.5</a>. <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p>Test short note. <a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3 role=doc-endnote><p>We use white and green in our physicalizations of data from the Shakespeare and Company Project because those are the two main colors used in the site design. <img src=images/scp_colors_dark.png alt="color palette from Shakespeare and Company Project"> <a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4 role=doc-endnote><p>Test note with image. <img src=images/icon-folding.svg alt="view folding section"> <a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5 role=doc-endnote><p>Test really long note. Viewers can see the rise and fall of subscribers, inspect the difference between active borrowers and subscribers, compare the growth over time by viewing the stacking part from the front, and survey the involvement of continuing members versus new patrons, to name a few possibilities. In some cases, a small number of new members were very active readers based on their borrowing activity. Data was mapped to the shapes with Data Illustrator, semi-manual calculation and vector drawing. <a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></div></div></article></main><section class=print-only><a class=first-page-header href=/ aria-label=Startwords><img alt=Startwords src=/pdf-logotype.svg></a>
<a class=page-header href=/ aria-label=Startwords><img alt=Startwords src=/pdf-logo.svg></a>
<a href=http://doi.org/10.5281/zenodo.3713671 rel=alternate class=page-doi>doi:10.5281/zenodo.3713671</a></section><footer><nav aria-label="footer links"><ul><li><a class=highlight-focus href=/about/>About</a></li></ul></nav><div class=icons><a class="license highlight-focus" rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons Attribution 4.0 International License" src=/img/logos/license.svg></a>
<a class=highlight-focus href=/ aria-label=Startwords><div class=logo id=startwords></div></a><a class=highlight-focus href=https://cdh.princeton.edu/ aria-label="The Center for Digital Humanities at Princeton"><div class=logo id=cdh></div></a></div></footer></body></html>