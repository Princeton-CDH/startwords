<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="https://gohugo.io/" version="0.101.0">Hugo</generator><title type="html">Startwords Issue Three</title><link href="https://startwords.cdh.princeton.edu/issues/3/" rel="alternate" type="text/html" title="HTML"/><link href="https://startwords.cdh.princeton.edu/issues/3/index.xml" rel="alternate" type="application/rss+xml" title="RSS"/><link href="https://startwords.cdh.princeton.edu/issues/3/atom.xml" rel="self" type="application/atom+xml" title="Atom"/><link href="https://startwords.cdh.princeton.edu/es/issues/3/" rel="alternate" type="text/html" hreflang="es" title="[es] HTML"/><link href="https://startwords.cdh.princeton.edu/es/issues/3/index.xml" rel="alternate" type="application/rss+xml" hreflang="es" title="[es] RSS"/><link href="https://startwords.cdh.princeton.edu/es/issues/3/atom.xml" rel="alternate" type="application/atom+xml" hreflang="es" title="[es] Atom"/><updated>2023-06-22T13:43:19+00:00</updated><rights>This work is licensed under a Creative Commons Attribution 4.0 International License.</rights><author><name>Center for Digital Humanities at Princeton</name><email>cdh-info@princeton.edu</email></author><id>https://startwords.cdh.princeton.edu/issues/3/</id><entry><title type="html">Are Large Language Models Our Limit Case?</title><link href="https://startwords.cdh.princeton.edu/issues/3/llm-limit-case/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://startwords.cdh.princeton.edu/issues/3/llm-limit-case/</id><author><name>Lauren Klein</name></author><published>2022-08-01T00:00:00+00:00</published><updated>2023-06-22T09:40:54-04:00</updated><content type="html"><![CDATA[<p>How is it possible to go forward with the development and use of large language models, given the clear evidence of just how biased, how incomplete, and how harmful &mdash; to both people and the planet &mdash; these models truly are? This is the question that &ldquo;Stochastic Parrots&rdquo; productively sets in motion, and that has continued to reverberate throughout the artificial intelligence (AI) and machine learning (ML) community since the paper&rsquo;s release.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<p>This is also a question &mdash; or, rather, a more specific version of a question &mdash; that I&rsquo;ve been thinking through for a number of years. In <em>Data Feminism</em>, for example, Catherine D&rsquo;Ignazio and I ask more broadly how datasets and data systems, which so often encode and amplify power differentials, might instead be reimagined so that they can challenge and rebalance those differentials.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> We document myriad instances of biased and incomplete datasets, as well as harmful and oppressive data systems that were conceived without attention to &mdash; let alone the involvement of &mdash; the communities most impacted by those systems.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> We also consider the environmental and economic costs of the computing infrastructure required to support such systems.<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> Yet we maintain a guarded optimism that data science, if intentionally deployed, can be used to challenge unjust power systems. Throughout the book, we advance a view that it still may be possible to remake the field of data science by building coalitions across communities and taking differential power into account, so as to wield the power of data with care and in the service of justice. The seven principles of data feminism that we describe in the book &mdash; examine power, challenge power, rethink binaries and hierarchies, elevate emotion and embodiment, embrace pluralism, consider context, and make labor visible &mdash; were intended to structure this transformative work.<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup></p>

<blockquote class="pull right">
    Are LLMs simply too big, and too ethically, environmentally, and epistemologically compromised, for humanities scholars to abide?
</blockquote>
<p>But when &ldquo;Stochastic Parrots&rdquo; was released, and when Dr. Gebru and Dr. Mitchell were subsequently fired &mdash; and then subjected to so much harassment and defamation online &mdash; it called into question the degree to which the transformative data science that Catherine and I had described in the book was truly possible.<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> With the opportunity to reengage with &ldquo;Stochastic Parrots,&rdquo; as this roundtable has invited us to do, I&rsquo;ve returned to this question of degree, which &mdash; one year later &mdash; now seems more accurately described as a question of bounds: Are large language models (LLMs) our limit case? In other words, are LLMs simply too big, and too ethically, environmentally, and epistemologically compromised, for humanities scholars to abide?</p>
<p>I will admit that I do not yet have a definitive answer to this question. But as a way of working through my current thinking, I will offer three unequivocal assertions:</p>
<div class="interlude">
<ol>
<li>There is no outside of unequal power.</li>
<li>All technologies are imbricated in this unequal power.</li>
<li>Refusal is, in itself, a generative act.</li>
</ol>

</div>
<p>In what follows, I elaborate each of these points.</p>
<hr>
<p><strong>First, there is no outside of unequal power</strong>. This is why an attention to power matters so much for discussions of data and models: it overdetermines not only the data we can collect and the datasets we can access, but also the research questions that we can explore. In <em>Data Feminism</em>, Catherine and I demonstrate how the financial and computational resources required to collect and analyze data at scale result in efforts most often undertaken by large corporations (and other well-resourced institutions) for their own profit and benefit, and at the expense of everyone else. This is why, to take one famous example that we discuss in the book, Target is able to analyze its own customer data in order to predict whether or not a person is pregnant &mdash; and then use that same data to sell them baby products; but there is not enough actual medical data that can be analyzed in order to predict whether that same pregnant person will be at risk of dying in childbirth.<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup> Or, to take an example discussed in &ldquo;Stochastic Parrots,&rdquo; why the Colossal Clean Crawled Corpus, even though it is less explicitly sexist, racist, and xenophobic than the Common Crawl corpus from which it is derived, remains sexist and racist and xenophobic in more subtle ways: because it was created by filtering out documents from the Common Crawl corpus that contained one or more words drawn from a list of &ldquo;<a href="https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words">Dirty, Naughty, Obscene, or Otherwise Bad Words</a>,&rdquo; it also excludes all discussions <em>about</em> those words &mdash; including direct critiques of those words or, in certain cases, discussions by those who might want to reclaim them.<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup> More recent research by Gururangan et al. has shown that even more sophisticated quality filters, such as the classifier employed to cull the dataset on which GPT-3 is trained, demonstrate significant stylistic as well as thematic preferences, preferences which correlate with a number of proxies for high socioeconomic status.<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup></p>
<p>Literary and historical corpora are not exempt from the influence of unequal power. We all know (or should know) that the <a href="https://www.hathitrust.org/">HathiTrust corpus</a>, because it is drawn from the collections of major research libraries, reflects the collecting preferences of those libraries (and the preferences of several libraries in particular) rather than literary production writ large.<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup> More domain-specific corpora, which would seem to avoid some of these issues as a result of their narrower scope, nonetheless continue to fall prey to the politics of digitization, as Katherine Bode and others have astutely observed.<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup> And those of us who work on texts related to the history of slavery, in particular, have long had to reckon with the politics of the archive itself: the fact that the first-hand accounts that might offer us the most direct access to the lives of the enslaved scarcely exist at all is because of the historical structures of power that control not only what enters the archive but also who is authorized to even write. That there is no outside of this unequal power is the starting point for our work. We must build our datasets and our models with our eyes wide open to this fact. As we acknowledge what we might learn from any new analyses, we must continue to account for the archive&rsquo;s &ldquo;null values,&rdquo; as Jessica Marie Johnson describes them: spaces held open for the people and stories that we know to have existed in their own time, even as they remain unknown to us in the present.<sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup></p>

<blockquote class="pull left">
    Academic researchers, especially at public institutions, are functionally if not ideologically compelled to concede to this asymmetrical configuration of power and resources.
</blockquote>
<p>Which brings me to my second point: <strong>all technologies are imbricated in this unequal power</strong>. Just as literary and other humanities scholars are deeply attuned to how power shapes both datasets and models, so too are we aware of how technology &mdash; all technology &mdash; is shaped by power as well. One could point to broad histories of computing, statistics, or surveillance for evidence of this fact.<sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup> But we could also look to work that has explored the imbrications of power in specific natural language processing (NLP) and ML techniques themselves. Work by Jeff Binder, for example, has located the origins of topic modeling in the need for U.S. intelligence agencies to quickly scan international newswires for potential geopolitical conflicts.<sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup> Melanie Walsh, for another, has recently reminded us that the annotated <a href="https://catalog.ldc.upenn.edu/LDC2013T19">OntoNotes corpus</a>, on which <a href="https://spacy.io/">spaCy&rsquo;s language models</a> were trained, was developed with funding by the U.S. Department of Defense&rsquo;s Defense Advanced Research Projects Agency (DARPA) &mdash; as is so much of NLP/ML/AI work today.<sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup> This circles back to a point that Catherine and I make in <em>Data Feminism</em>, about unequal power being the result of unequal resources. This point is made in &ldquo;Stochastic Parrots&rdquo; as well: because these technologies are so resource-dependent, not only in terms of energy, cost, and computing power, but also because of very specific technical expertise, they are necessarily developed by people at elite and well-resourced institutions who are rarely required &mdash; either by inclination or circumstance &mdash; to take power into account.</p>
<p>Meredith Whittaker, another former Google employee forced out because of her labor-organizing efforts there, observes as much in a recent essay that documents the &ldquo;capture&rdquo; of supervised machine learning algorithms (including large language models) by Big Tech.<sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup> Whittaker explains how any gains in performance they might achieve are the result not of any major algorithmic or architectural innovation, but rather, of what existing algorithms can do &ldquo;when matched with large-scale data and computational resources.”<sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup> These are resources that only big tech firms control. So when academic researchers seek to engage with these developments, they find themselves beholden to the very same tech firms for the data and computing infrastructure, and very often funding, as a necessary precondition for contributing on an equal plane. The systematic defunding of higher education that has taken place over the past several decades is not the focus of Whittaker&rsquo;s piece, but this issue is the other side of the same neoliberal coin. Academic researchers, especially at public institutions, are functionally if not ideologically compelled to concede to this asymmetrical configuration of power and resources as a result of the federal, state, and institutional policies that have transferred the responsibility of supporting and sustaining research (in terms of computing infrastructure, student support, and even their own salaries) to the scholars themselves. And all of this is to say nothing of corporations like Facebook that are actively and intentionally wielding their data and algorithms to retain their own power, even as they know full well the harms that their products produce.<sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup></p>
<p>Given this ethical, intellectual, and economic double-bind, it&rsquo;s not surprising that the most clear and compelling response may be to refuse &mdash; to refuse to develop new models, to refuse to improve existing ones, or even to refuse to participate in this work altogether. This brings me to my third assertion: that <strong>refusal is, in itself, a generative act</strong>. I&rsquo;ve long admired the work of Dr. Joy Buolamwini and the <a href="https://www.ajl.org/">Algorithmic Justice League</a> (AJL), for example. This work began (in work coauthored with Dr. Gebru) by identifying biases in the image datasets used to train three major gender classification software libraries.<sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup> But when the initial audit found significant error rates in how the software classified images of women, and images of dark-skinned women in particular, Buolamwini&rsquo;s response was <em>not</em> to suggest that the training data be &ldquo;debiased&rdquo; or otherwise improved.<sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup> Rather, because she recognized how improved gender classification software (and facial recognition software more generally) would most likely be used to increase the policing and surveillance of Black and brown people, she used the evidence of her paper with Gebru to instead call for a ban on facial recognition software altogether.<sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup></p>
<p>In terms of text analysis tools more specifically, I&rsquo;ve followed with interest the recent actions by the team behind ml5.js, the JavaScript-based machine learning library, which upon discovering racist language in its sample word2vec model, around which its documentation is based, decided to remove it (and as a result, render the entire library nonfunctional) until an alternate model could be trained.<sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup> In both of these cases, the AJL and ml5.js, we see evidence of how refusal &mdash; especially when accompanied (as feminists advise) by a recommitment to values, or action, or both &mdash; can clear the space to imagine alternate possibilities.<sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup></p>
<p>&ldquo;Stochastic Parrots&rdquo; participates in this reimagining by describing an alternative approach to technical research, one in which issues of cost, access, potential harms, and potential benefits are addressed early in the research process. This slower and more intentional process also allows for input from &mdash; and, ideally, meaningful collaboration with &mdash; impacted communities. This process echoes some of what Catherine and I have described as data science for good vs. data science for co-liberation: the latter imagines a way of doing data science in which those from both dominant and minoritized groups work together to free themselves from the oppressive systems that harm all of us.<sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup></p>
<div class="interlude">
<table> <caption>Above: Features of ‘data for good’ versus data for co-liberation, from Catherine D’Ignazio and Lauren F. Klein, Data Feminism (MIT Press, 2020), p. 140.</caption>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:center">&ldquo;Data for good&rdquo;</th>
<th style="text-align:center">Data for co-liberation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Leadership by members of minoritized groups working in community</td>
<td style="text-align:center"></td>
<td style="text-align:center">✓</td>
</tr>
<tr>
<td style="text-align:left">Money and resources managed by members of minoritized groups</td>
<td style="text-align:center"></td>
<td style="text-align:center">✓</td>
</tr>
<tr>
<td style="text-align:left">Data owned and governed by the community</td>
<td style="text-align:center"></td>
<td style="text-align:center">✓</td>
</tr>
<tr>
<td style="text-align:left">Quantitative data analysis “ground truthed” through a participatory, community-centered data analysis process</td>
<td style="text-align:center"></td>
<td style="text-align:center">✓</td>
</tr>
<tr>
<td style="text-align:left">Data scientists are not rock stars and wizards, but rather facilitators and guides</td>
<td style="text-align:center"></td>
<td style="text-align:center">✓</td>
</tr>
<tr>
<td style="text-align:left">Data education and knowledge transfer are part of the project design</td>
<td style="text-align:center"></td>
<td style="text-align:center">✓</td>
</tr>
<tr>
<td style="text-align:left">Building social infrastructure—community solidarity and shared understanding—is part of the project design</td>
<td style="text-align:center"></td>
<td style="text-align:center">✓</td>
</tr>
</tbody>
</table>

</div>
<p>But the discussion of LLMs at the center of &ldquo;Stochastic Parrots&rdquo; complicates this vision of data for co-liberation in necessary ways, because LLMs may well function as a limit case. This is for a number of reasons. For one, the computational resources required to train up larger and larger models may make it infrastructurally impossible to allow anyone from outside of these institutions &mdash; let alone members of minoritized groups &mdash; to assume primary leadership of these models&rsquo; training or future development. The same holds for financial leadership, since LLMs are just as resource-intensive with respect to cost as they are to compute. It is difficult to see how any corporation &mdash; which, by definition, is driven by its own bottom line &mdash; would allow an outside group to independently manage a budget that reflected one of these models&rsquo; true economic cost, even as that same corporation might bestow seemingly generous grants to outside groups for specific purposes.</p>
<p>Along with questions about resources are questions about the models themselves. As the authors of &ldquo;Stochastic Parrots&rdquo; spell out, the predictive power of LLMs derives, in large part, from the size of the datasets used to train them. Is it possible for a single community, or even a consortium of impacted groups, to own and govern the increasing amount of training data that is required to train a new model from the ground up? Furthermore, without explainability mechanisms co-designed by communities, rather than by computer scientists alone, how might the results of any particular LLM-based data analysis project be ground-truthed by the community members themselves?</p>
<p>There are additional ethical questions that arise as a result of this range of dependencies. Is it possible for corporations and their own technical workers to participate in a process of knowledge transfer while remaining uncompromised by their profit-driven agenda? How can academic researchers, if brought into this process, ensure that their own values remain uncompromised? What alternative infrastructures &mdash; social, technical, financial, or governmental &mdash; must be imagined such that Big Tech does not remain a required research partner? And even if removed from the corporate sphere, can LLMs &mdash; defined as they are by their size and scale &mdash; ever be enlisted in the necessarily slow, careful, and localized work of building solidarity and shared understanding?</p>

<blockquote class="pull right">
    How can we reconcile the historical specificity that we so value in our own research with the fact that even the most appropriate LLM for historical scholarship may be trained on data so temporally distant from the time period that bounds our own scholarly expertise?
</blockquote>
<p>Quantitative literary and cultural studies scholars can continue to learn from the work that the authors of &ldquo;Stochastic Parrots&rdquo; have undertaken since the paper&rsquo;s release (and even since the roundtable which prompted these remarks), which addresses many of these issues head on. For example, in December 2021, Dr. Gebru announced the launch of DAIR, the Distributed AI Research Institute, which is guided by a commitment to &ldquo;ensur[ing] that researchers from many different backgrounds can participate while embedded in their communities.”<sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup> Just a few months earlier, in August 2021, Dr. Mitchell announced that she would be joining Hugging Face, a startup that is working to provide open-source alternatives to the language models produced by the Big Five, as well as software libraries which increase access to existing models and examples of documentation and other best practices.<sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup> Meanwhile, McMillan-Major has continued her work on best practices for documenting the datasets employed in NLP research,<sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup> while Dr. Bender has continued to draw attention to the limits of LLMs as well as their potential harms, both through publications aimed at the AI, ML, and NLP research communities and in comments addressed to the public at large.<sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup></p>
<p>But certain questions that pertain to humanistic inquiry in particular, both methodological and epistemological, remain to be addressed by quantitative literary and cultural studies scholars. For example, even as there begin to exist LLMs that are trained on historical corpora, the amount of data that is required results in training datasets with timespans &mdash; 1450 to 1950, in the case of MacBERTh &mdash; that far exceed any disciplinary sense of periodization.<sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup> How can we reconcile the historical specificity that we so value in our own research with the fact that even the most appropriate LLM for historical scholarship may be trained on data so temporally distant from the time period that bounds our own scholarly expertise? Furthermore, even as we know to fine-tune such a model on our own more curated datasets, how are we to measure the effects of that fine-tuning in ways that are meaningful to us as humanities scholars? When parameters no longer correspond to specific textual or linguistic features, as with earlier model architectures, we will require even more creative ways to understand the significance of the texts contained in our curated datasets in relation to those on which the larger model was trained.</p>
<p>In addition, we must consider how decades of feminist thinking &mdash; and, for that matter, much of the most profound of humanities scholarship &mdash; have confirmed how a single voice at the margins can tell us just as much as (if not more than) a large group at the center. How do we hold fast to this fact as the allure of LLMs, enlisted as they are in the service of &ldquo;shared&rdquo; or generalizable tasks, continues to mount? How can we envision methods to engage these models in ways that center marginalized voices and the texts that document them? How can we amplify rather than merely assimilate the important oppositional ideas that these texts record? And how can we do so while remaining mindful of the ideas &mdash; and the people behind them &mdash; that these models cannot or at times should not subsume?</p>
<p>The answers to these questions we might, in turn, bring back to the authors of &ldquo;Stochastic Parrots,&rdquo; augmenting their vision of the reimagining of large language models and their required infrastructures that must necessarily take place. As humanities scholars, we also must recommit to showing how literary, cultural, and historical context not only enriches our present understanding of LLMs, but is required for all future model-based research. After all, this is the set of contexts from which large language models emerged, and it is only with a deep knowledge of these contexts that we can fully understand their uses and limits.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Emily M. Bender et al., &ldquo;On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ,&rdquo; <em>FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</em> (New York: Association for Computing Machinery, 2021), 610&ndash;23, <a href="https://doi.org/10.1145/3442188.3445922">https://doi.org/10.1145/3442188.3445922</a>. On &ldquo;community&rdquo; as an empty signifier, especially in the AI ethics space, see J. Khadijah Abdurahman, &ldquo;Holding to Account: Safiya Umoja Noble and Meredith Whittaker on Duties of Care and Resistance to Big Tech,&rdquo; <em>Logic Magazine</em>, Dec. 2021, <a href="https://logicmag.io/beacons/holding-to-account-safiya-umoja-noble-and-meredith-whittaker/">https://logicmag.io/beacons/holding-to-account-safiya-umoja-noble-and-meredith-whittaker/</a>.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Catherine D&rsquo;Ignazio and Lauren F. Klein, <em>Data Feminism</em> (Cambridge: MIT Press, 2020), <a href="https://data-feminism.mitpress.mit.edu/">https://data-feminism.mitpress.mit.edu/</a>.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>See D&rsquo;Ignazio and Klein, <em>Data Feminism</em>, chap. 1, <a href="https://data-feminism.mitpress.mit.edu/pub/vi8obxh7/release/4;">https://data-feminism.mitpress.mit.edu/pub/vi8obxh7/release/4;</a> and chap. 5, <a href="https://data-feminism.mitpress.mit.edu/pub/2wu7aft8/release/3">https://data-feminism.mitpress.mit.edu/pub/2wu7aft8/release/3</a>.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>See D&rsquo;Ignazio and Klein, <em>Data Feminism</em>, chap. 1, <a href="https://data-feminism.mitpress.mit.edu/pub/vi8obxh7/release/4">https://data-feminism.mitpress.mit.edu/pub/vi8obxh7/release/4</a> ; and chap. 7, <a href="https://data-feminism.mitpress.mit.edu/pub/0vgzaln4/release/3/">https://data-feminism.mitpress.mit.edu/pub/0vgzaln4/release/3/</a>.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>See D&rsquo;Ignazio and Klein, introduction to <em>Data Feminism</em>, <a href="https://data-feminism.mitpress.mit.edu/pub/frfa9szd/release/6">https://data-feminism.mitpress.mit.edu/pub/frfa9szd/release/6</a>.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>On Dr. Gebru&rsquo;s firing, see Cade Metz and Daisuke Wakabayashi, &ldquo;Google Researcher Says She was Fired Over Paper Highlighting Bias In A.I.,&rdquo; <em>New York Times</em>, Dec. 3, 2020, <a href="https://www.nytimes.com/2020/12/03/technology/google-researcher-timnit-gebru.html">https://www.nytimes.com/2020/12/03/technology/google-researcher-timnit-gebru.html</a>; on Dr. Mitchell’s firing, see Cade Metz, “A Second Google A.I. Researcher Says the Company Fired Her,” <em>New York Times</em>, Feb. 19, 2021, <a href="https://www.nytimes.com/2021/02/19/technology/google-ethical-artificial-intelligence-team.html">https://www.nytimes.com/2021/02/19/technology/google-ethical-artificial-intelligence-team.html</a>; on the harassment that followed, see Zoe Schiffer, &ldquo;Timnit Gebru Was Fired from Google &mdash; Then the Harassment Arrived,&rdquo; <em>The Verge</em>, Mar. 5, 2021, <a href="https://www.theverge.com/22309962/timnit-gebru-google-harassment-campaign-jeff-dean">https://www.theverge.com/22309962/timnit-gebru-google-harassment-campaign-jeff-dean</a>. For Catherine&rsquo;s response to Dr. Gebru&rsquo;s firing in particular, see Katlyn Turner, Danielle Wood, and Catherine D&rsquo;Ignazio, &ldquo;The Abuse and Misogynoir Playbook,&rdquo; in <em>The State of AI Ethics Report</em> (Montreal AI Ethics Institute, 2021), 14&ndash;34, <a href="https://montrealethics.ai/wp-content/uploads/2021/01/The-State-of-AI-Ethics-Report-January-2021.pdf#page=15">https://montrealethics.ai/wp-content/uploads/2021/01/The-State-of-AI-Ethics-Report-January-2021.pdf#page=15</a>.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>D&rsquo;Ignazio and Klein, <em>Data Feminism</em>, chap. 1, <a href="https://data-feminism.mitpress.mit.edu/pub/vi8obxh7/release/4">https://data-feminism.mitpress.mit.edu/pub/vi8obxh7/release/4</a>.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Bender et al., &ldquo;Stochastic Parrots,&rdquo; 614.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Suchin Gururangan et al., &ldquo;Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection&rdquo;, arXiv, updated Jan. 26, 2022, <a href="https://arxiv.org/abs/2201.10474">https://arxiv.org/abs/2201.10474</a>.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Benjamin Schmidt, &ldquo;What&rsquo;s in the HathiTrust?&rdquo; <em>Sapping Attention</em> (blog), Feb. 3, 2022, <a href="https://sappingattention.blogspot.com/2019/03/whats-in-hathi-trust.html">https://sappingattention.blogspot.com/2019/03/whats-in-hathi-trust.html</a>.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Katherine Bode, &ldquo;Why You Can&rsquo;t Model Away Bias,&rdquo; <em>Modern Language Quarterly</em> 81, no. 1 (2020): 95&ndash;124. For an example and analysis of the politics of digitization in action, see Benjamin Fagan, &ldquo;Chronicling White America,&rdquo; <em>American Periodicals</em> 26, no. 1 (2016): 10&ndash;13.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Jessica Marie Johnson, <em>Wicked Flesh: Black Women, Intimacy, and Freedom in the Atlantic World</em> (Philadelphia: Univ. of Pennsylvania Press, 2020).&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>See Simone Browne, <em>Dark Matters: On the Surveillance of Blackness</em> (Durham: Duke Univ. Press, 2015); Mar Hicks, <em>Programmed Inequality: How Britain Discarded Women Technologists and Lost Its Edge in Computing</em> (Cambridge: MIT Press, 2017); and Banu Subramaniam, <em>Ghost Stories for Darwin: The Science of Variation and the Politics of Diversity</em> (Urbana: Univ. of Illinois Press, 2014).&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>Jeffrey M. Binder, &ldquo;Alien Reading: Text Mining, Language Standardization, and the Humanities,&rdquo; in <em>Debates in the Digital Humanities 2016</em>, ed. Matthew K. Gold and Lauren F. Klein (Minneapolis: Univ. of Minnesota Press, 2016), <a href="https://dhdebates.gc.cuny.edu/read/untitled/section/4b276a04-c110-4cba-b93d-4ded8fcfafc9">https://dhdebates.gc.cuny.edu/read/untitled/section/4b276a04-c110-4cba-b93d-4ded8fcfafc9</a>.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>Melanie Walsh, <em>Introduction to Cultural Analytics and Python,</em> version 1.1.0 (Aug. 31, 2021), <a href="https://doi.org/10.5281/zenodo.4411250">https://doi.org/10.5281/zenodo.4411250</a>.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p>On retaliation against Whittaker, see Nitasha Tiku, &ldquo;Google Walkout Organizers Say They&rsquo;re Facing Retaliation,&rdquo; <em>Wired</em>, Apr. 2, 2019, <a href="https://www.wired.com/story/google-walkout-organizers-say-theyre-facing-retaliation/">https://www.wired.com/story/google-walkout-organizers-say-theyre-facing-retaliation/</a>.&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Meredith Whittaker, &ldquo;The Steep Cost of Capture,&rdquo; <em>ACM Interactions,</em> vol. 28, no .6 (Nov.&ndash;Dec. 2021): 50, <a href="https://interactions.acm.org/archive/view/november-december-2021/the-steep-cost-of-capture">https://interactions.acm.org/archive/view/november-december-2021/the-steep-cost-of-capture</a>.&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18">
<p>See, for example, Karen Hao, &ldquo;The Facebook Whistleblower Says Its Algorithms Are Dangerous. Here&rsquo;s Why,&rdquo; <em>MIT Technology Review,</em> Oct. 5, 2021, <a href="https://www.technologyreview.com/2021/10/05/1036519/facebook-whistleblower-frances-haugen-algorithms/">https://www.technologyreview.com/2021/10/05/1036519/facebook-whistleblower-frances-haugen-algorithms/</a>; and Karen Hao, &ldquo;She Risked Everything to Expose Facebook. Now She&rsquo;s Telling Her Story,&rdquo; <em>MIT Technology Review</em>, July 29, 2021, <a href="https://www.technologyreview.com/2021/07/29/1030260/facebook-whistleblower-sophie-zhang-global-political-manipulation/">https://www.technologyreview.com/2021/07/29/1030260/facebook-whistleblower-sophie-zhang-global-political-manipulation/</a>.&#160;<a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19">
<p>Joy Buolamwini and Timnit Gebru, &ldquo;Intersectional Accuracy Disparities in Commercial Gender Classification,&rdquo; <em>Proceedings of Machine Learning Research</em> 81 (2018): 77­&ndash;91. <a href="http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf">http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf</a>&#160;<a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20">
<p>For the audit, see Joy Buolamwini and Timnit Gebru, &ldquo;Gender Shades,&rdquo; accessed May 15, 2022, <a href="http://gendershades.org/index.html">http://gendershades.org/index.html</a>.&#160;<a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21">
<p>See &ldquo;Policy/Advocacy,&rdquo; Algorithmic Justice League, accessed May 15, 2022, <a href="https://www.ajl.org/library/policy-advocacy">https://www.ajl.org/library/policy-advocacy</a>.&#160;<a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22">
<p>See @ml5js, &ldquo;The team recently learned that the example models used to demonstrate our word2vec functionality include racial slurs and other offensive words,&rdquo; Twitter, Oct. 6, 2021, 10:46 a.m., <a href="https://twitter.com/ml5js/status/1445762321444315147">https://twitter.com/ml5js/status/1445762321444315147</a>.&#160;<a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23">
<p>On feminist refusal, see Marika Cifor et al., &ldquo;Feminist Data Manifest-No&rdquo; (2019), <a href="https://www.manifestno.com/">https://www.manifestno.com/</a>; and Patricia Garcia et al., &ldquo;No: Critical Refusal as Feminist Data Practice,&rdquo; abstract, in <a href="https://dl.acm.org/doi/proceedings/10.1145/3406865"><em>CSCW &lsquo;20 Companion: Conference Companion Publication of the 2020 on Computer Supported Cooperative Work and Social Computing</em></a> (New York: Association for Computing Machinery, 2020), 199&ndash;202, <a href="https://doi.org/10.1145/3406865.3419014">https://doi.org/10.1145/3406865.3419014</a>.&#160;<a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24">
<p>D&rsquo;Ignazio and Klein, <em>Data Feminism</em>, chap. 5, <a href="https://data-feminism.mitpress.mit.edu/pub/2wu7aft8/release/3%5C#nobxi408tlj">https://data-feminism.mitpress.mit.edu/pub/2wu7aft8/release/3\#nobxi408tlj</a>.&#160;<a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25">
<p>&ldquo;Research Philosophy,&rdquo; Distributed Artificial Intelligence Research Institute, accessed May 15, 2022, <a href="https://www.dair-institute.org/research">https://www.dair-institute.org/research</a>.&#160;<a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26">
<p>For reportage on Hugging Face&rsquo;s most recent round of funding, see, for example, Romain Dillet, &ldquo;Hugging Face Reaches $2 Billion Valuation to Build the GitHub of Machine Learning,&rdquo; <em>TechCrunch</em>, May 9, 2022, <a href="https://techcrunch.com/2022/05/09/hugging-face-reaches-2-billion-valuation-to-build-the-github-of-machine-learning/">https://techcrunch.com/2022/05/09/hugging-face-reaches-2-billion-valuation-to-build-the-github-of-machine-learning/</a>.&#160;<a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27">
<p>See, for example, Angela McMillan-Major, Emily Bender, and Batya Friedman, &ldquo;Data Statements: Documenting the Datasets Used for Training and Testing Natural Language Processing Systems&rdquo; (poster, Scholarly Communication in Linguistics: Resource Workshop and Poster Session, Linguistic Society of America, virtual, Jan. 6, 2022), <a href="https://www.linguisticsociety.org/system/files/abstracts/summary/Scholarly%20Communication%20in%20Linguistics.pdf">https://www.linguisticsociety.org/system/files/abstracts/summary/Scholarly%20Communication%20in%20Linguistics.pdf</a>.&#160;<a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28">
<p>See, for example, Emily M. Bender, &ldquo;On NYT Magazine on AI: Resist the Urge to Be Impressed,&rdquo; <em>Medium</em> (blog), Apr. 17, 2022, <a href="https://medium.com/@emilymenonbender/on-nyt-magazine-on-ai-resist-the-urge-to-be-impressed-3d92fd9a0edd">https://medium.com/@emilymenonbender/on-nyt-magazine-on-ai-resist-the-urge-to-be-impressed-3d92fd9a0edd</a>; Steven Johnson, &ldquo;A.I. is Mastering Language. Should We Trust What It Says?&rdquo; <em>New York Times Magazine</em>, Apr. 17, 2022, <a href="https://www.nytimes.com/2022/04/15/magazine/ai-language.html">https://www.nytimes.com/2022/04/15/magazine/ai-language.html</a>; and Inioluwa Deborah Raji et al., &ldquo;AI and the Everything in the Whole Wide World Benchmark,&rdquo; <em>NeurIPS</em> 2021, <a href="https://arxiv.org/abs/2111.15366">https://arxiv.org/abs/2111.15366</a>.&#160;<a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29">
<p>Enrique Manjavacas and Lauren Fonteyn, &ldquo;MacBERTh: Development and Evaluation of a Historically Pre-trained Language Model for English (1450&ndash;1950), in <em>Proceedings of the Workshop on Natural Language Processing for Digital Humanities (NLP4DH)</em> (Stroudsburg: Association of Computing and the Humanities, 2021), 23&ndash;36, <a href="https://rootroo.com/downloads/nlp4dh_proceedings_draft.pdf">https://rootroo.com/downloads/nlp4dh_proceedings_draft.pdf</a>. The project page currently lives at <a href="https://macberth.netlify.app/">https://macberth.netlify.app/</a>.&#160;<a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">Mapping the Latent Spaces of Culture</title><link href="https://startwords.cdh.princeton.edu/issues/3/mapping-latent-spaces/?utm_source=atom_feed" rel="alternate" type="text/html"/><id>https://startwords.cdh.princeton.edu/issues/3/mapping-latent-spaces/</id><author><name>Ted Underwood</name></author><published>2022-08-01T00:00:00+00:00</published><updated>2023-06-22T09:40:54-04:00</updated><content type="html"><![CDATA[<p>The technology I need to discuss in this paper doesn&rsquo;t yet have a consensus name. Some observers point to an architecture, the Transformer.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> &ldquo;On the Dangers of Stochastic Parrots&rdquo; focuses on size and discusses &ldquo;large language models.”<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> A paper from Stanford emphasizes applications: &ldquo;foundation models&rdquo; are those that can adapt &ldquo;to a wide range of downstream tasks.”<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> Each name identifies a different feature of recent research as the one that matters. To keep that question open, I&rsquo;ll refer here to &ldquo;deep neural models of language,&rdquo; a looser category.</p>
<p>However we define them, neural models of language are already changing the way we search the web, write code, and even play games. Academics outside computer science urgently need to reflect on them. &ldquo;On the Dangers of Stochastic Parrots&rdquo; deserves credit for starting that discussion &mdash; especially since its publication required tenacity and courage. I am honored to be part of a forum exploring its significance for the humanities.</p>

<blockquote class="pull right">
    In historical disciplines, it is far from obvious that all meaning boils down to intentional communication between individuals.
</blockquote>
<p>The argument that Bender et al. advance has two parts: first, that large language models pose social risks, and second, that they will turn out to be a &ldquo;misdirected research effort&rdquo; anyway, since they pretend to perform &ldquo;natural language understanding&rdquo; but &ldquo;do not have access to meaning.&rdquo; (615).</p>
<p>I agree that the trajectory of recent research has risks. But to understand the risks language models pose, I think we will need to understand how they produce meaning. The premise that they simply &ldquo;do not have access to meaning&rdquo; tends to prevent us from seeing the models’ social role. I hope humanists can help illuminate that role by offering a wider range of ways to think about the work language does.</p>
<h2 id="language-models-as-models-of-culture">Language models as models of culture</h2>
<p>It is true that language models don&rsquo;t yet represent their own communicative goals or an interlocutor&rsquo;s state of mind. These are important aspects of language, and for &ldquo;Stochastic Parrots,&rdquo; they are the whole story: the article defines <em>meaning</em> as &ldquo;meaning conveyed between individuals&rdquo; and &ldquo;grounded in communicative intent&rdquo; (616).</p>
<p>But in historical disciplines, it is far from obvious that all meaning boils down to intentional communication between individuals. Historians often use <em>meaning</em> to describe something more collective, because the meaning of a literary work, for example, is not circumscribed by intent. It is common for debates about the meaning of a text to depend more on connections to books published a century earlier (or later) than on reconstructing the author&rsquo;s conscious plan.<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup></p>
<p>I understand why researchers in a field named &ldquo;artificial intelligence&rdquo; would associate meaning with mental activity and see writing as a dubious proxy for it. But historical disciplines rarely have access to minds, or even living subjects. We work mostly with texts and other traces. For this reason, I&rsquo;m not troubled by the part of &ldquo;Stochastic Parrots&rdquo; that warns about &ldquo;the human tendency to attribute meaning to text&rdquo; even when the text &ldquo;is not grounded in communicative intent&rdquo; (618, 616). Historians are already in the habit of finding meaning in genres, nursery rhymes, folktale motifs, ruins, political trends, and other patterns that never had a single author with a clear purpose.<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> If we could find meaning only in intentional communication, we wouldn&rsquo;t find much meaning in the past at all. So not all historical researchers will be scandalized when we hear that a model is merely &ldquo;stitching together sequences of linguistic forms it has observed in its vast training data&rdquo; (617). That&rsquo;s what we often do too, and we could use help.</p>
<p>A willingness to find meaning in collective patterns may be especially necessary for disciplines that study the past. But this flexibility is not limited to scholars. The writers and artists who borrow language models for creative work likewise appreciate that their instructions to the model acquire meaning from a training corpus. The phrase &ldquo;Unreal Engine,&rdquo; for instance, encourages a neural network called <a href="https://openai.com/blog/clip/">CLIP</a> to select pictures with a consistent, cartoonified style. But this has nothing to do with the dictionary definition of &ldquo;unreal.&rdquo; It&rsquo;s just a helpful side-effect of the fact that many video game screenshots are captioned with the name of the game engine (Unreal Engine) that produced them.</p>

<blockquote class="pull left">
    Intelligence only starts to interest us after it mixes with time to become a biased, limited pattern of collective life.
</blockquote>
<p>In short, I think people who use neural models of language typically use them for a different purpose than &ldquo;Stochastic Parrots&rdquo; assumes. The immediate value of these models is often not to mimic individual language understanding, but to represent specific cultural practices (like styles or expository templates) so they can be studied and creatively remixed. This may be disappointing for disciplines that aspire to model general intelligence. But for historians and artists, cultural specificity is not disappointing. Intelligence only starts to interest us after it mixes with time to become a biased, limited pattern of collective life. Models of culture are exactly what we need.</p>
<h2 id="language-models-in-historical-research">Language models in historical research</h2>
<p>While I&rsquo;m skeptical that language models are devoid of meaning, I do share other concerns raised by the authors of &ldquo;Stochastic Parrots.&rdquo; For instance, I agree that researchers will need a way to understand the subset of texts that shape a model&rsquo;s response to a given prompt. Culture is historically specific, so models will never be free of omission and bias. But by the same token, we need to know which practices they represent.</p>
<p>If companies want to offer language models as a service to the public &mdash; say, in web search &mdash; they will need to do even more than know what their models represent. Somehow, a single model will need to produce a picture of the world that is acceptable to a wide range of audiences, without amplifying harmful biases or filtering out minority discourses (Bender et al., &ldquo;Stochastic Parrots,&rdquo; 614). That&rsquo;s a delicate balancing act.</p>
<p>Historians don&rsquo;t have to compress their material as severely as that. Since history is notoriously a story of conflict, and our sources were interested participants, few people expect historians to represent all aspects of the past with one correctly balanced model. On the contrary, historical inquiry is usually about comparing perspectives. Machine learning is not the only way to do this, but it can help. For instance, researchers can measure differences of perspective by training multiple models on different publication venues or slices of the timeline.<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup></p>
<p>When research is organized by this sort of comparative purpose, the biases in data are not usually a reason to refrain from modeling &mdash; but a reason to create more corpora and train models that reflect a wider range of biases. On the other hand, training a variety of models becomes challenging when each job requires thousands of GPUs. Tech companies might have the resources to train many models at that scale. But will universities?</p>
<p>One way around this impasse is to train a single model that can explicitly distinguish multiple perspectives. At present, researchers create this flexibility in a rough and ready way by &ldquo;fine-tuning&rdquo; BERT on different samples. A more principled approach might design models to recognize the social structure in their original training data. One recent paper associates each text with a date stamp, for instance, to train models that respond differently to questions about different years.<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup> Similar approaches might produce models explicitly conditioned on variables like venue or nationality &mdash; models that could associate each statement or prediction they make with a social vantage point. Producing models that can represent hundreds of vantage points may become easier if this technology evolves &mdash; as increasingly seems likely &mdash; to separate the language model proper from a larger database that explicitly encodes world knowledge.<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup> Training a new language model is computationally expensive. But editing the knowledge base to reflect a particular period or set of sources might be relatively cheap.</p>

<blockquote class="pull right">
    Neural models more closely resemble movable type: they will change the way culture is transmitted in many social contexts.
</blockquote>
<p>If neural language models are to play a constructive role in research, universities will also need alternatives to material dependence on tech giants. In 2020, it seemed that only the largest corporations could deploy enough silicon to move this field forward. In October 2021, things are starting to look less dire. Coalitions like EleutherAI are reverse-engineering language models.<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup> Smaller corporations like Hugging Face are helping to cover underrepresented languages. NSF is proposing new computing resources.<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup> The danger of oligopoly is by no means behind us, but we can at least begin to see how scholars might train models that represent a wider range of perspectives.</p>
<h2 id="the-effects-of-modeling-culture">The effects of modeling culture</h2>
<p>Of course, scholars are not the only people who matter. What will language models (and models of culture) mean for people outside universities?</p>
<p>I agree with the authors of &ldquo;Stochastic Parrots&rdquo; that neural language models are dangerous. But I am not sure that critical discourse has alerted us to the most important dangers yet. Critics often prefer to say that these models are dangerous only because they don&rsquo;t work and are devoid of meaning. Perhaps that seems to be the strongest rhetorical position, since it concedes no value to the models. But I suspect this hard line also prevents critics from envisioning what the models might be good for and how they&rsquo;re likely to be (mis)used.</p>
<p>Consider the surprising art scene that sprang up when CLIP was released. OpenAI still hasn&rsquo;t released the DALL-E model that uses the numbers CLIP assigns to text to find a corresponding point in a latent space of hypothetical images.<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup> But that didn&rsquo;t stop graduate students and interested amateurs from duct-taping CLIP to various generative image models and using the contraption to explore visual culture in dizzying ways.<sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup></p>
<p>



























<figure ><img loading="lazy" alt="A computer generated picture of a candle. The automatically generated text description reads: a picture containing candle." src="/issues/3/mapping-latent-spaces/images/AngelOfAirUnrealKomatsuzaki.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/issues/3/mapping-latent-spaces/images/AngelOfAirUnrealKomatsuzaki_huba09b4d92ba08e95ffd4d773ee920e94_588572_500x0_resize_box_3.png 500w,
    /issues/3/mapping-latent-spaces/images/AngelOfAirUnrealKomatsuzaki_huba09b4d92ba08e95ffd4d773ee920e94_588572_800x0_resize_box_3.png 800w,/issues/3/mapping-latent-spaces/images/AngelOfAirUnrealKomatsuzaki.png 640w" 
     class="landscape"
     ><figcaption>
        <p>The angel of air. Unreal Engine.<span class="attribution">VQGAN + CLIP, Aran Komatsukaki, May 31, 2021.</span>
        </p>
    </figcaption>
</figure>
<div class="txt-only">
#&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;
| FIGURE.  A computer generated picture of a candle. The automatically generated text description reads: a picture containing candle.
|
| CAPTION: The angel of air. Unreal Engine.
| ATTRIBUTION: VQGAN + CLIP, Aran Komatsukaki, May 31, 2021.
| LINK:
#&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;
</div></p>
<p>Does the emergence of this subculture make any sense if we assume that CLIP is just a failed attempt to reproduce individual language use? In practice, the people tinkering with CLIP don&rsquo;t expect it to respond like a human reader. More to the point, they don&rsquo;t want it to. They&rsquo;re fascinated because CLIP uses language <em>differently</em> than a human individual would &mdash; mashing together the senses and overtones of words and refracting them into the potential space of internet images like a new kind of synesthesia.<sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup> The pictures produced are fascinating, but (at least for now) too glitchy to impress most people as art. They&rsquo;re better understood as postcards from an unmapped latent space.<sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup> The point of a postcard, after all, is not to be itself impressive, but to evoke features of a larger region that looks fun to explore. Here the &ldquo;region&rdquo; is a particular visual culture; artists use CLIP to find combinations of themes and styles that could have occurred within it (although they never quite did).</p>
<p>



























<figure ><img loading="lazy" alt="A picture containing text, water, ocean, shore. Description automatically generated." src="/issues/3/mapping-latent-spaces/images/UnderwoodClockworkangelrockycoast.png"
     sizes="(max-width: 768px) 100vw, 80vw"
     srcset="/issues/3/mapping-latent-spaces/images/UnderwoodClockworkangelrockycoast_hu08092fee10768487a4ec3ee3d3c37133_943675_500x0_resize_box_3.png 500w,
    /issues/3/mapping-latent-spaces/images/UnderwoodClockworkangelrockycoast_hu08092fee10768487a4ec3ee3d3c37133_943675_800x0_resize_box_3.png 800w,/issues/3/mapping-latent-spaces/images/UnderwoodClockworkangelrockycoast.png 815w" 
     class="landscape"
     ><figcaption>
        <p>The clockwork angel of air flying over a rocky coast, Kodak Portra film.<span class="attribution">Ted Underwood, using Katherine Crowson&rsquo;s cc12m_1 diffusion model, December 28, 2021.</span>
        </p>
    </figcaption>
</figure>
<div class="txt-only">
#&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;
| FIGURE.  A picture containing text, water, ocean, shore. Description automatically generated.
|
| CAPTION: The clockwork angel of air flying over a rocky coast, Kodak Portra film.
| ATTRIBUTION: Ted Underwood, using Katherine Crowson&rsquo;s cc12m_1 diffusion model, December 28, 2021.
| LINK:
#&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;
</div></p>
<p>Will models of this kind also have negative effects? Absolutely. The common observation that they could reinforce existing biases is the mildest possible example. If we approach neural models as machines for mapping and rewiring collective behavior, we will quickly see that they could do much worse than reinforce existing biases: for instance, deepfakes could create new hermetically sealed subcultures and beliefs that are difficult to contest.</p>
<p>My goal in this essay is not to decide whether neural language models are good or bad &mdash; just to clarify what&rsquo;s being modeled, why people care, and what kinds of (good or bad) effects we might expect. Reaching a comprehensive judgment is likely to take decades. After all, models are easy to distribute. So this was never a problem, like gene splicing, that could stay bottled up as an ethical dilemma for one profession that controlled the tools. Neural models more closely resemble movable type: they will change the way culture is transmitted in many social contexts. Since the consequences of movable type included centuries of religious war in Europe, my analogy is not meant to reassure. I just mean that questions on this scale don&rsquo;t get resolved quickly or by experts. We are headed rather for a broadly political debate about antitrust, renewable energy, and the shape of human culture itself &mdash; a debate where everyone will have some claim to expertise.<sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup></p>
<p>Let me end, however, on a positive note. I have suggested that approaching neural models as models of culture rather than intelligence gives us even more reason to worry about them. But it also gives us more reason to hope. It is not entirely clear what we plan to gain by modeling intelligence, since there are already more than seven billion intelligences on the planet. By contrast, it is easy to see how exploring spaces of possibility implied by the past of human culture could support a more reflective and more adventurous approach to our future. I can imagine a world where generative models of culture are used grotesquely or locked down as IP for Netflix. But I can also imagine a world where fan communities use them to remix plot tropes and gender norms, making &ldquo;mass culture&rdquo; a more self-conscious, various, and participatory phenomenon than the twentieth century usually allowed it to become.</p>
<p>I don&rsquo;t know which of those worlds we will build. But either way, I suspect we will need to reframe our conversation about artificial intelligence as a conversation about models of culture and the latent spaces they imply. Philosophers and science fiction writers may enjoy debating whether software can have mental attributes like intention. But that old argument does little to illuminate the social questions new technologies are really raising. Neural language models are dangerous and fascinating because they can illuminate and transform shared patterns of behavior &mdash; in other words, cultural practices. When the problem is redescribed this way, the concerns about equity foregrounded by &ldquo;Stochastic Parrots&rdquo; still matter deeply. But the imagined contrast between mimicry and meaning in the article&rsquo;s title no longer connects with any satirical target. Culture clearly has meaning. But I&rsquo;m not sure that anyone cares whether a culture has autonomous intent, or whether it is merely parroting human action.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Ashish Vaswani et al., &ldquo;Attention Is All You Need,&rdquo; 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, 2017. <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Emily M. Bender et al., &ldquo;On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ,&rdquo; <em>FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</em> (New York: Association for Computing Machinery, 2021), 610&ndash;623, <a href="https://doi.org/10.1145/3442188.3445922">https://doi.org/10.1145/3442188.3445922</a>. Further citations of this work are given in the text.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Rishi Bommasani et al., &ldquo;On the Opportunities and Risks of Foundation Models,&rdquo; CoRR Aug 2021, 3&ndash;4, <a href="https://arxiv.org/abs/2108.07258">https://arxiv.org/abs/2108.07258</a>.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>&ldquo;It is language which speaks, not the author.&rdquo; Roland Barthes, &ldquo;The Death of the Author,&rdquo; in <em>Image / Music / Text</em>, trans. Stephen Heath (New York: Hill and Wang, 1977), 143.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>To this list one might also add the material and social aspects of book production. In commenting on &ldquo;Stochastic Parrots,&rdquo; Katherine Bode notes that book history prefers to paint a picture where &ldquo;meaning is dispersed across . . . human and non-human agents.&rdquo; Katherine Bode, qtd. in Lauren M. E. Goodlad, &ldquo;Data-Centrism and Its Discontents,&rdquo; <em>Critical AI</em> (blog)<em>,</em> Oct. 15, 2021, <a href="https://criticalai.org/2021/10/14/blog-recap-stochastic-parrots-ethics-of-data-curation/">https://criticalai.org/2021/10/14/blog-recap-stochastic-parrots-ethics-of-data-curation/</a>.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Sandeep Soni, Lauren F. Klein, and Jacob Eisenstein, &ldquo;Abolitionist Networks: Modeling Language Change in Nineteenth-Century Activist Newspapers,&rdquo; <em>Journal of Cultural Analytics</em> 6, no. 1 (Jan. 18, 2021), <a href="https://culturalanalytics.org/article/18841-abolitionist-networks-modeling-language-change-in-nineteenth-century-activist-newspapers">https://culturalanalytics.org/article/18841-abolitionist-networks-modeling-language-change-in-nineteenth-century-activist-newspapers</a>; Ted Underwood, &ldquo;Machine Learning and Human Perspective,&rdquo; <em>PMLA</em> 135, no. 1 (Jan. 2020): 92&ndash;109, <a href="http://hdl.handle.net/2142/109140">http://hdl.handle.net/2142/109140</a>.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Bhuwan Dhingra et al., &ldquo;Time-Aware Language Models as Temporal Knowledge Bases,&rdquo; <em>Transactions of the Association for Computational Linguistics</em> 10 (Mar. 18, 2022): 257&ndash;73, <a href="https://doi.org/10.1162/tacl_a_00459">https://doi.org/10.1162/tacl_a_00459</a>.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Sebastian Borgeaud et al., &ldquo;Improving Language Models by Retrieving from Trillions of Tokens,&rdquo; arXiv, Dec. 8, 2021, <a href="https://arxiv.org/abs/2112.04426v1">https://arxiv.org/abs/2112.04426v1</a>.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>See for instance, Sid Black et al., &ldquo;GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow,&rdquo; version 1.0, Mar. 21, 2021, <a href="https://doi.org/10.5281/zenodo.5297715">https://doi.org/10.5281/zenodo.5297715</a>.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>White House Briefing Room, &ldquo;The White House Announces the National Artificial Intelligence Research Resource Task Force,&rdquo; June 10, 2021, <a href="https://www.whitehouse.gov/ostp/news-updates/2021/06/10/the-biden-administration-launches-the-national-artificial-intelligence-research-resource-task-force/">https://www.whitehouse.gov/ostp/news-updates/2021/06/10/the-biden-administration-launches-the-national-artificial-intelligence-research-resource-task-force/</a>.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Aditya Ramesh et al., &ldquo;Zero-Shot Text-to-Image Generation,&rdquo; arXiv, updated Feb. 26, 2021, <a href="https://arxiv.org/abs/2102.12092">https://arxiv.org/abs/2102.12092</a>.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>One early technique was eventually published as Katherine Crowson et al., &ldquo;VQGAN-CLIP: Open Domain Image Generation and Editing with Natural Language Guidance,&rdquo; arXiv, Apr. 18, 2022, <a href="https://arxiv.org/abs/2204.08583">https://arxiv.org/abs/2204.08583</a>.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>One good history of this scene is titled &ldquo;Alien Dreams&rdquo;&mdash;a title that concisely indicates how little interest artists have in using CLIP to reproduce human behavior. Charlie Snell, &ldquo;Alien Dreams: An Emerging Art Scene,&rdquo; <em>Machine Learning at Berkeley</em> (blog), June 30, 2021, <a href="https://ml.berkeley.edu/blog/posts/clip-art/">https://ml.berkeley.edu/blog/posts/clip-art/</a>.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>For a skeptical history of this spatial metaphor, see Nick Seaver, &ldquo;Everything Lies in a Space: Cultural Data and Spatial Reality,&rdquo; in &ldquo;Towards an Anthropology of Data,&rdquo; ed. Rachel Douglas-Jones, Antonia Walford, and Nick Seaver, special issue, <em>Journal of the Royal Anthropological Institute</em> 27, no. 1 (Apr. 2021): 43&ndash;61, <a href="https://doi.org/10.1111/1467-9655.13479">https://doi.org/10.1111/1467-9655.13479</a>. We also skeptically probe the limits of spatial metaphors for culture (but end up confirming their value) in Ted Underwood and Richard Jean So, &ldquo;Can We Map Culture?&rdquo; <em>Journal of Cultural Analytics</em> 6, no. 3 (June 17, 2021), <a href="https://doi.org/10.22148/001c.24911">https://doi.org/10.22148/001c.24911</a>.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>I haven&rsquo;t said much about the energy cost of training models. For one thing, I&rsquo;m not fully informed about contemporary efforts to keep that cost low. More importantly, I think the cause of carbon reduction is actively harmed by pitting different end users against each other. If we weigh the &ldquo;carbon footprint&rdquo; of your research agenda against my conference travel, the winner will almost certainly be British Petroleum. Renewable energy is a wiser thing to argue about if carbon reduction is actually our goal. For another version of this idea, see Mark Kaufman, &ldquo;The Carbon Footprint Sham: A &lsquo;Successful, Deceptive&rsquo; PR Campaign,&rdquo; <em>Mashable,</em> July 9, 2021, <a href="https://mashable.com/feature/carbon-footprint-pr-campaign-sham">https://mashable.com/feature/carbon-footprint-pr-campaign-sham</a>.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry><entry><title type="html">On Spanish-Speaking Parrots</title><link href="https://startwords.cdh.princeton.edu/issues/3/on-spanish-parrots/?utm_source=atom_feed" rel="alternate" type="text/html"/><link href="https://startwords.cdh.princeton.edu/es/issues/3/on-spanish-parrots/?utm_source=atom_feed" rel="alternate" type="text/html" hreflang="es"/><id>https://startwords.cdh.princeton.edu/issues/3/on-spanish-parrots/</id><author><name>Gimena del Rio Riande</name></author><published>2022-08-01T00:00:00+00:00</published><updated>2023-06-22T09:40:54-04:00</updated><content type="html"><![CDATA[<h2 id="1-a-parrot-called-maria">1. A Parrot Called MarIA</h2>
<p>Spanish is the second most widely spoken language in the world as a mother tongue. Official reports, survey-based studies, and Wikipedia confirm it. And Google can predict it.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> The data speaks for itself: there are more than twenty countries where Spanish is the official language. At United States universities, Spanish is the most popular second language choice, and there are twenty-three Academies of the Spanish Language &mdash; Academias de la Lengua Española &mdash; all over the world.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> But Spanish has been losing the geopolitical and symbolic battle against English as the language of science since the last century, and few technological developments have helped to improve this situation.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></p>
<p>In recent years, global research on artificial intelligence (AI) and machine learning has grown exponentially, offering computational linguistics and natural language processing (NLP) a place of relevance in the academic curriculum of different disciplines and fields, such as digital humanities. Nonetheless, the North-South imbalance in this field is evident. Most NLP research is still primarily done in English, and it takes a lot of time to make these resources available in Spanish. Even if they do become available, it is often via multilingual versions that are not as accurate as the English alternative.</p>

<blockquote class="pull right">
    The so-called Global South is suspicious of technology, as an uncomfortable consumer of foreign Northern developments.
</blockquote>
<p>Among Spanish-speaking countries, Spain has been taking the lead in AI and NLP research with official initiatives like <a href="https://datos.gob.es/en/report"><em>Aporta</em></a>, published in the Ministry of Economic Affairs and Digital Transformation&rsquo;s portal, and their project, &ldquo;Tecnologías emergentes y datos abiertos: Inteligencia Artificial.&rdquo;<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> Although the Spanish initiative is welcome and reflects a sustained human and technological work, it takes an absolutely technopositivist approach. There is not much critical reflection on the dangers or limitations in the use of open public data to develop and exploit language models, and nothing is said, for instance, about the rare-earth technologies that are part of a strategic industry and a geopolitical asset that only a few countries in the world take profit from.</p>
<p>Last July, the Iberian country unveiled the first major project on language and AI from the National Library of Spain (Biblioteca Nacional de España — BNE). MarIA is the first massive AI model of the Spanish language. MarIA, a RoBERTa model, was born from a large amount of data that the BNE ingested in the MareNostrum supercomputer of the Barcelona Supercomputing Centre. MarIA&rsquo;s data are the files in WARC format resulting from the tracking and archiving of the Spanish website, which, by law, the BNE scrapes and preserves.<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup></p>
<p>Let&rsquo;s talk about numbers: 59 terabytes of the BNE web archive and 6,910,000 hours of the MareNostrum supercomputer were used to build, curate, and compile this corpus. As a result, 201,080,084 clean and duplicate-free documents were obtained, occupying a total of 570 gigabytes. The second step of the training, based on neural network technology, required 184,000 processor hours and more than 18,000 GPU hours.<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> According to the paper published a few months ago, the released models &mdash; between 125 million and 355 million parameters respectively &mdash; will be expanded using new and different sources, such as scientific publications of the Spanish Higher Council for Scientific Research (Consejo Superior de Investigaciones Científicas — CSIC) and the Spanish Wikipedia.<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup> There are also plans to start training models for other Romance languages ​​like Catalan, Galician, Basque, and Portuguese and for much more complex varieties of Spanish, such as what is usually termed Latin American Spanish or <em>español de América</em>.</p>
<p>It comes as no surprise that the so-called Global South is suspicious of technology, as an uncomfortable consumer of foreign Northern developments and as a receiver of critiques from the big nations.<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup> A primary example is the accusation that Southern nations do little to reduce environmental damage by perpetuating the use of old technologies, a charge that distracts from discussions on the ways that new bitcoin, blockchain, and AI industries are polluting and damaging the Global South.<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup></p>
<p>I must confess that my perspective on language models and AI is one of a Southern researcher who is quite suspicious both of MarIA&rsquo;s Spanish and its capabilities, especially when it adapts its performance, for instance, to the <em>rioplatense</em> Spanish in translation apps, subtitling, chatbots, and automatic language prediction or correction.<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup> I find this situation to be an example of what Thomas Hervé Mboa Nkoudou defines as the techno-utopian rhetoric that trumpets the benefits of technological innovations but, paradoxically, rarely refers to the risks or drawbacks associated with the adoption of socio-technical infrastructures.<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup></p>
<p>I am convinced that we need more MarIAs. Still, I think these MarIAs should be explained in an open research ecosystem if they really want to overcome the problems of techno-colonialism and become reusable, reproducible models in those more than twenty Spanish-speaking countries. I am not aware of the research performed by the BNE model in terms of linguistic, geographic, or racial bias, but I imagine that the Spanish Language Academies around the world should also take part in this project. When it comes to the release of a model to the Spanish-speaking world for mass adoption, openness and diversity should be mandatory.</p>
<h2 id="2-a-parrot-called-bertin">2. A Parrot called BERTIN</h2>
<p>The same week MarIA was widely announced in the media, BERTIN was born.<sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup> The aim of this project was to pre-train a RoBERTa-base model from scratch using Common Crawl during the Flax/JAX Community Event, held July 7–14, 2021. I was impressed when, all of a sudden, there were two RoBERTa models available in Spanish. I decided to interview one of its mentors, <a href="https://scholar.google.com/citations?user=31VBFVIAAAAJ&amp;hl=en">Dr. Javier de la Rosa</a>, a young and brilliant Spanish researcher and NLP expert. In the following section, part of the conversation we had is transcribed since it can help us reflect on how, why, and which language models are needed for the Spanish language.</p>
<h3 id="21-javier-bertin-and-gimena">2.1. Javier, BERTIN, and Gimena</h3>
<p><strong>Gimena (G):</strong> <em>BERTIN is proposed as a collaborative project. What role do humanists or linguists play (or could they play) in creating datasets and curating outputs?</em></p>
<p><strong>Javier (J):</strong> The creation of BERTIN has been community-oriented from its inception, made open by and for the community. Programmers, engineers, AI researchers, digital humanists, and computational linguists showed keen interest in taking part in BERTIN. Unfortunately, due to a purely practical matter (we only had funding and resources for ten days), not everyone was able to participate. However, we had very interesting and rich conversations about the orientation and goals of the project. In that sense, I think that one of the aspects both humanists and linguists could contribute to in these kinds of projects is the detection and documentation of bias. In our case, this issue emerged as an afterthought. As we analyzed the model, we realized that it preferred words from European or peninsular Spanish (&ldquo;coche&rdquo; vs &ldquo;auto&rdquo; vs &ldquo;carro&rdquo; ), and we understood that the model could suffer from certain geographic biases that had to be highlighted and corrected. Unfortunately, it is not easy to decrease bias after the model is trained. Nonetheless, team members with a linguistic background did a very good job documenting it.</p>

<blockquote class="pull left">
    Not all biases are bad. For example, one way to have a language model that is capable of understanding other varieties of Spanish could be skewing the dataset in order to magnify a less represented variety.
</blockquote>
<p><strong>G:</strong> <em>How should we create linguistic data or datasets in such a way that they do not perpetuate biases and hegemonic norms?</em></p>
<p><strong>J:</strong> At the moment, AI in general and language models in particular are very active fields. Toolkits are starting to analyze the data before the training begins, for example, to reduce certain known biases or to find patterns that could become problematic when exploiting the models. However, I must say that not all biases are bad. For example, one way to have a language model that is capable of understanding other varieties of Spanish could be skewing the dataset in order to magnify a less represented variety and produce a model capable of adapting better to diverse varieties.</p>
<p><strong>G:</strong> <em>If you had to teach a course on data curation from a humanistic perspective, what methods or practices would you teach? Can we imagine a future where the humanities become important to data experts?</em></p>
<p>It is true that humanists have been curating data from time immemorial, but we are lagging behind and moving too slowly compared to the advances of technology and AI. That being said, there are initiatives to make these two worlds a bit more data-savvy. For example, <a href="https://collectionsasdata.github.io/">Thomas Padilla&rsquo;s Collections as Data project</a> has opened a new research line for institutions, especially libraries and archives. This can be seen in language models and other AI initiatives such as the one from the National Library of Norway, which used its digital collection to build a language model for Norwegian, including that of the BNE.<sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup> Therefore, I believe that if I had to teach data curation, I would do it from this hybrid perspective, pointing out the importance of preservation and the need to have data in formats that are not only interoperable but also (re)usable.</p>
<p><strong>G:</strong> <em>Do you see negative consequences of humanists relying on Google either to use or test their models? Is there a chance that we can develop open source resources for this kind of research? Should we continue to trust big tech companies with questionable ethical practices, or start developing smaller language models?</em></p>
<p>Business for large companies does not rely on software, but on hardware and infrastructure. Most of the advances in AI are developed using open technologies, and this is why it has progressed so fast during the last 10–15 years. Both the adoption of free software and the release of models and open source libraries are ways of inviting users to work on those platforms. For instance, if you want to train a model for Basque, you can do it in Google Cloud and then leave. But if Google provides you with the code and allows you to integrate it with its platform, and also teaches you how to apply it in thirty other languages, it saves you a lot of work. Of course, you can also try to train your model on a supercomputer like the MareNostrum that has been used for the BNE model, but access to these types of resources is not easy or direct. You could even buy 120 NVIDIA graphic cards and train them, but it is a material investment difficult to justify when there are on-demand solutions that assure an efficient use of money. What we have tried to show with BERTIN is that some costs can be reduced when training a model. We are still a long way from being able to train these massive models on our personal computers, but we must not stop making progress in this regard.</p>
<h2 id="3-some-reflections">3. Some reflections</h2>
<p>From my point of view, an important conclusion that emerges from the BERTIN project is that, although we cannot escape the need for big data and big machines to produce language models with good performance, the expense of training them can be reduced in terms of time and data. That makes it possible for smaller teams to enter a domain that, for the moment, is the exclusive preserve of large Global Northern companies and institutions. It is also a proposal aware of the needs of a more open approach to research and of the environmental impacts of technology. Far from a techno-utopian perspective, BERTIN is not at war with companies or institutions and it is not aligned with open science activism. It somehow tries to take the best of both worlds. Possibly, this middle ground is where global Spanish AI and NLP projects could take place in the future years.</p>

<blockquote class="pull right">
    Language models . . . can help generate automated text summaries, find similarities in textual collections, classify works based on theme, genre, or content, and link data based only on information contained in a text.
</blockquote>
<p>Although the field of AI goes beyond the horizon of humanistic work, it is evident that language models have many applications with an immediate impact on the humanities and digital humanities. They can improve the outputs of systems for optical character recognition (OCR), stylometry, and authorial attribution. They can help generate automated text summaries, find similarities in textual collections, classify works based on theme, genre, or content, link data based only on information contained in a text, etc. Still, it is important to note that the outputs of these systems must also be subjected to study and criticism, aligning, in this way, with open science initiatives. In this regard, another project by Javier De la Rosa I would like to refer to is ALBERTI, a BERT-based multilingual model for poetry. ALBERTI is capable of completing poems automatically. But in a second stage, humans evaluate and determine which ALBERTI substitutions could be considered more poetic.<sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup> It&rsquo;s a good example of how all models must be evaluated and limited by humans, not only in the humanities but in everyday life. As the proverb goes, &ldquo;Quien mucho habla, mucho yerra&rdquo; (&ldquo;He who speaks too much, makes many mistakes&rdquo;), which seems true both for humans and machines.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>The title and content of this essay play with that of the article by Emily M. Bender et al., &ldquo;On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?,&rdquo; FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (New York: Association for Computing Machinery, 2021), 610–23, <a href="https://doi.org/10.1145/3442188.3445922">https://doi.org/10.1145/3442188.3445922</a>. This essay and &ldquo;Sobre los loros que hablan español&rdquo; do not have a direct translation relationship. Initially, when I was invited to participate in the virtual round table &ldquo;Machine Predictions and Synthetic Text&rdquo;, which was held on October 26, 2021, and in which I participated together with outstanding specialists such as Lauren Klein, Ted Underwood, Toma Tasovac and two of the authors of the aforementioned article (Angelina McMillan-Major and Margaret Mitchell), I wrote the text in Spanish, in order to organize my ideas. A week before the event, I rewrote the text in English, trying to escape (with medium or little luck) the insurmountable problem of transferring grammatical structures and expressions from Spanish to English. I would like to thank my dear sister, María Cruz del Rio, for correcting the first English draft and Grant Wythoff and David Rivera for final review.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>According to a report published by the Cervantes Institute, 528 million people speak Spanish as a native, second or foreign language. Cervantes Institute, &ldquo;El español, una lengua que hablan 580 millones de personas, 483 millones de ellos nativos,&rdquo; Oct. 15, 2019, <a href="https://www.cervantes.es/sobre_instituto_cervantes/prensa/2019/noticias/presentacion_anuario_madrid.htm">https://www.cervantes.es/sobre_instituto_cervantes/prensa/2019/noticias/presentacion_anuario_madrid.htm</a>.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>In this regard, a very interesting multilingual tool for text discoverability that could be adapted to different disciplines, created by an Argentine biologist and an American bioinformatician, is <a href="https://panlingua.rxivist.org">PanLingua</a>. PanLingua allows searches in the user's language on the bioRxiv.org database using Google Translate to provide automatic translations of the query term into different languages.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>For an English version of the documentation of this project, see Alejandro Alija, <em>Emerging Technologies and Open Data: Artificial Intelligence</em> (Iniciativa aporta, 2020), <a href="https://datos.gob.es/en/documentacion/emerging-technologies-and-open-data-artificial-intelligence">https://datos.gob.es/en/documentacion/emerging-technologies-and-open-data-artificial-intelligence</a>.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Gutiérrez Fandiño et al. do not mention whether the large amount of literary text digitized by the BNE was used to feed MarIA. Asier Gutiérrez Fandiño et al., &ldquo;MarIA: Spanish Language Models,&rdquo; arXiv, updated Apr. 5, 2022, <a href="https://arxiv.org/abs/2107.07253">https://arxiv.org/abs/2107.07253</a>,&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Gutiérrez Fandiño et al, &ldquo;MarIA,&rdquo; <a href="https://arxiv.org/abs/2107.07253">https://arxiv.org/abs/2107.07253</a>,&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>However, they don&rsquo;t specify whether &ldquo;the Spanish Wikipedia&rdquo; points to the Wikipedia of Spain or of the different Spanish-speaking Wikipedias.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>For a very interesting initiative on Global South AI, see Ranjit Singh, &ldquo;Mapping AI in the Global South,&rdquo; Data and Society: Points (blog), Jan. 26, 2021, <a href="https://points.datasociety.net/ai-in-the-global-south-sites-and-vocabularies-e3b67d631508">https://points.datasociety.net/ai-in-the-global-south-sites-and-vocabularies-e3b67d631508</a>.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Peter Howson, &ldquo;Climate Crises and Crypto-Colonialism: Conjuring Value on the Blockchain Frontiers of the Global South,&rdquo; <em>Frontiers in Blockchain</em> 3 (May 2020), <a href="https://doi.org/10.3389/fbloc.2020.00022">https://doi.org/10.3389/fbloc.2020.00022.</a> For an explanation of how the bitcoin industry is causing power outages in Argentina, see &ldquo;Échale la culpa a Bitcoin (por cortes de luz): el Gobierno apunta a empresas de minería ilegales,&rdquo; iProUP, Dec. 31, 2021, <a href="https://www.iproup.com/finanzas/28621-cortes-de-luz-el-gobierno-busca-granjas-de-minado-de-bitcoin">https://www.iproup.com/finanzas/28621-cortes-de-luz-el-gobierno-busca-granjas-de-minado-de-bitcoin</a>.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p><em>Rioplatense</em> is the variety of Spanish spoken mainly in Argentina and Uruguay. Wikipedia, s.v. &ldquo;Rioplatense Spanish,&rdquo; last modified Apr. 30, 2022, 14:12, <a href="https://en.wikipedia.org/wiki/Rioplatense_Spanish">https://en.wikipedia.org/wiki/Rioplatense_Spanish</a>.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Thomas Hervé Mboa Nkoudou, &ldquo;Les makerspaces en Afrique francophone, entre développement local durable et technocolonialité: trois études de cas au Burkina Faso, au Cameroun et au Sénégal&rdquo; (PhD diss., Université Laval, 2020), <a href="https://corpus.ulaval.ca/jspui/handle/20.500.11794/67577">https://corpus.ulaval.ca/jspui/handle/20.500.11794/67577</a>.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>For more about BERTIN, see &ldquo;Bertin-roberta-base-spanish,&rdquo; Hugging Face, updated Apr. 28, 2022, <a href="https://huggingface.co/bertin-project/bertin-roberta-base-spanish">https://huggingface.co/bertin-project/bertin-roberta-base-spanish</a>.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Author&rsquo;s note: For a description of the project, see Kummervold et al., &ldquo;Operationalizing a National Digital Library: The Case for a Norwegian Transformer Model,&rdquo; <em>Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)</em>, ed. Simon Dobnik and Lilja Øvrelid (Linköping Univ. Electronic Press, 2021), 20–29, <a href="https://aclanthology.org/2021.nodalida-main.pdf">https://aclanthology.org/2021.nodalida-main.pdf</a>.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>For more about ALBERTI, see &ldquo;ALBERTI vs BERT,&rdquo; Hugging Face, accessed May 15, 2022, <a href="https://huggingface.co/spaces/flax-community/alberti">https://huggingface.co/spaces/flax-community/alberti</a>.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content></entry></feed>