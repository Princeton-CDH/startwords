<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#3D206C"><meta http-equiv=x-ua-compatible content="IE=edge"><link rel=alternate hreflang=en href=https://startwords.cdh.princeton.edu/issues/3/><link rel=alternate hreflang=es href=https://startwords.cdh.princeton.edu/es/issues/3/><link rel=apple-touch-icon sizes=180x180 href=/img/favicon/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/img/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/img/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=384x384 href=/img/favicon/android-chrome-384x384.png><link rel=icon type=image/png sizes=192x192 href=/img/favicon/android-chrome-192x192.png><link rel=icon type=image/png sizes=150x150 href=/img/favicon/mstile-150x150.png><link rel="shortcut icon" href=/favicon.ico><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/img/favicon/safari-pinned-tab.svg color=#3d206c><link rel=preload as=font type=font/woff2 href=/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-regular.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-regular.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-300.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-700.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-300italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-500italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-700italic.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/IBM_Plex_Serif/ibm-plex-serif-v8-latin-500.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/Source_Sans_Pro/source-sans-pro-v13-latin-regular.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/Source_Sans_Pro/source-sans-pro-v13-latin-900.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/Source_Sans_Pro/source-sans-pro-v13-latin-300.woff2 crossorigin><title>Issue Three</title><meta name=description content="A research periodical irregularly published by the Center for Digital Humanities at Princeton."><meta property="og:title" content="Issue Three"><meta property="og:description" content="A research periodical irregularly published by the Center for Digital Humanities at Princeton."><meta property="og:type" content="website"><meta property="og:url" content="https://startwords.cdh.princeton.edu/issues/3/"><meta property="og:image" content="https://startwords.cdh.princeton.edu/social.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://startwords.cdh.princeton.edu/social.png"><meta name=twitter:title content="Issue Three"><meta name=twitter:description content="A research periodical irregularly published by the Center for Digital Humanities at Princeton."><link rel=stylesheet href=/style.css><link rel=stylesheet href=/print.css media=print><script src=/js/polyfills.js></script><script defer src=/js/bundle.js></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0XGPQFYSR7"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-0XGPQFYSR7",{anonymize_ip:!1,cookie_domain:"startwords.cdh.princeton.edu",cookie_flags:"SameSite=None;Secure"})}</script><link rel=alternate type=application/rss+xml href=/issues/3/index.xml><link rel=alternate type=application/atom+xml href=/issues/3/atom.xml></head><body class=issue><header><nav class=main aria-label=main><ul><li class=home><a href=/><img class=logo src=/img/logos/startwords.svg alt=Home width=15 height=33></a></li><li class=issues><a href=/issues/>Issues</a></li></ul></nav></header><main><div class="issue-summary grid"><div class=container><span class=number>Issue 3</span>
<time class=pubdate datetime=2022-08>August 2022</time><h1 class=theme>Parrots</h1><div class=summary><p>By the time Emily Bender, Timnit Gebru, Angelina McMillan-Major and Margaret Mitchell&rsquo;s paper &ldquo;<a href=https://doi.org/10.1145/3442188.3445922>On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?</a>&rdquo; was published in March 2021, it had already been shaking up the artificial intelligence (AI) world for some time. Two of its authors were <a href=https://www.emergingtechbrew.com/stories/2021/03/29/one-biggest-advancements-ai-also-sparked-fierce-debate-heres>fired from Google</a>, and the natural language processing (NLP) community was grappling with the uncomfortable truths the paper had raised about the harms posed by algorithms that many claimed would change modern life. Those of us working in the computational humanities certainly took notice of the Parrots paper, too. While most humanists’ day-to-day research tasks may not directly involve large language models (LLMs), the concerns raised by the authors &mdash; about the ethical, social, and environmental risks of emerging technologies &mdash; were familiar terrain. The paper also helped us revisit the humanistic principles we espouse in our approaches to text, language, and meaning, and to consider what happens when they intersect with tools emerging from a tech culture obsessed with chasing what is biggest and fastest.</p><p>The history of NLP in the humanities goes back longer than most people would think. There was a time &mdash; in the sixties or seventies, during the early days of humanities computing and the heyday of Chomskian linguistics &mdash; where NLP more or less made sense to your average humanities scholar. This was because NLP was to a large extent rule-based. Computer scientists and linguists built complex systems on top of explicit rules (and exceptions, of course) that would, in theory at least, cover all the grammatical and syntactical structures of natural language. These systems could then be used in automatic translation, text summarization, question and answering, and similar tasks. Rule-based NLP made intuitive sense to humanists because this is how many of us &mdash; or at least those of us of a certain generation who went to school before the internet &mdash; learned foreign languages: by learning and following explicit rules. If you think of grammar as a system of declensions and conjugations that can be mastered only by group recitation and hard-core drilling, that means you&rsquo;ve been around the block quite a few times.</p><p>But the dominant approach in NLP these days has nothing to do with explicit rules; instead, it is based on statistical models. Statistical NLP infers rules from existing texts and annotations by converting words into vectors in a multidimensional space. These &mdash; to the human mind &mdash; impenetrable models are used to make predictions on new data. This works fairly well for certain types of tasks. But it also feels a bit like magic.</p><p>Most humanists come to quantitative analysis with a healthy dose of skepticism to begin with: we are trained to recognize that context is everything, that meaning is always irreducibly complex, that texts are often inherently contradictory, and that there is no such thing as ideology-free space. So it should come as no surprise that humanists are especially sensitive to the challenges of power dynamics, data availability, and domain specificity, as well as structural and representational bias in statistical language models based on large datasets. As we watch statistical models transform not just data-driven research, but the way the world economy functions, the way our healthcare is managed, and the way we are policed, we see the human and human experience being pushed further and further to the margins.</p><p>While most of us nodded vigorously while reading the Parrots paper, we also knew that substantive discussion across the disciplinary boundaries of the humanities and data and computer science can feel insurmountable. How do we talk to each other in this age of intense academic overspecialization? How do we prevent our segregated disciplines from turning us into methodological, epistemological, and ideological loners? Is there a meaningful way to combine positivistic or empirical approaches to language with those rooted in humanistic idealism?</p><p>We wanted to bring varied perspectives together to discuss &ldquo;On the Dangers of Stochastic Parrots&rdquo; and consider how the humanistic view may help forge the path toward mitigating the risks posed by emerging technologies such as LLMs. We invited three leading digital humanists &mdash; Gimena del Rio Riande, Lauren Klein, and Ted Underwood &mdash; to share their thoughts, and asked two of the article co-authors &mdash; McMillan-Major and Mitchell &mdash; to respond during <a href=https://cdh.princeton.edu/updates/join-us-for-a-roundtable-on-machine-predictions-and-synthetic-text/>a live-streamed Zoom roundtable</a> in late October 2021.</p><p>The three humanists’ position papers comprise this issue of <em>Startwords</em>. In &ldquo;Mapping the Latent Spaces of Culture,&rdquo; Underwood acknowledges the serious risks posed by LLMs, but also asks us to take a broad view of how language functions, how models produce meaning, and how disruptive technologies have also always played a generative role in transforming cultural practices.</p><p>Del Rio Riande&rsquo;s piece, &ldquo;On Spanish-Speaking Parrots,&rdquo; examines the role of language by foregrounding another vexed problem in NLP, which is its lack of linguistic diversity. She discusses the BERTIN project, which not only produced a monolingual LLM for Spanish, but as a collaborative and community-driven effort, exemplified a more ethical alternative to the technopositivist and resource-intensive models that concern the Parrots authors. (The &ldquo;small batch&rdquo; approach to creating annotated data and training models for new languages has also been the approach taken by the CDH in our NEH-funded <a href=https://newnlp.princeton.edu/>New Languages for NLP: Building Linguistic Diversity in the Digital Humanities Institute</a>, a collaborative project with DARIAH-EU.)</p><p>Finally, in &ldquo;Are Large Language Models Our Limit Case?,&rdquo; Klein asks provocatively whether all of us &mdash; in industry, in academia, or in our personal lives &mdash; who keep finding ourselves trying to work around these asymmetrical configurations of power and resources, should perhaps embrace radical refusal and work toward changing our systems instead.</p><div class=authors>— 
<a href=/authors/#TasovacToma><address>Toma Tasovac</address></a>​ and
<a href=/authors/#ErmolaevNatalia><address>Natalia Ermolaev</address></a></div><div class=translators>Spanish translation by David Rivera</div></div><a href=#features class="chevron highlight-focus"><img src=/img/chevron.svg alt="skip to featured content"></a><aside class=translations><h2 class=sr-only>languages available</h2><ul><li class=current><span>EN</span></li><li><a href=/es/issues/3/ aria-label="read this page in Spanish" title="read this page in Spanish">ES</a></li></ul></aside></div></div><section id=features aria-label=features class="inverted grid"><div class=wide-container><article class=article-summary><div class=bg-text role=presentation><span class=shaper style=height:97.15px></span>The technology I need to discuss in this paper doesn’t yet have a consensus name.</div><div class="bg-text secondary" role=presentation><span class=shaper style=height:97.15px></span>The technology I need to discuss in this paper doesn’t yet have a consensus name.</div><p><span class=shaper style=height:97.15px></span>The technology I need to discuss in this paper doesn’t yet have a consensus name.</p><h2><a class=highlight-focus href=/issues/3/mapping-latent-spaces/>Mapping the Latent Spaces of Culture</a></h2><div class=languages aria-label='languages available'><span aria-label=English title="Available in English">EN</span></div><ul class=authors><li><address>Ted Underwood</address><a href=/authors/#UnderwoodTed aria-label="author info" title="author info"><i class=ph-link-thin></i></a></li></ul><a class="doi highlight-focus" href=https://doi.org/10.5281/zenodo.6567481>10.5281/zenodo.6567481</a></article><article class=article-summary><div class=bg-text role=presentation><span class=shaper style=height:180px></span>Spanish is the second most widely spoken language in the world as a mother tongue. Official reports, survey-based studies, and Wikipedia confirm it. And Google can predict it.</div><div class="bg-text secondary" role=presentation><span class=shaper style=height:180px></span>Spanish is the second most widely spoken language in the world as a mother tongue. Official reports, survey-based studies, and Wikipedia confirm it. And Google can predict it.</div><p><span class=shaper style=height:180px></span>Spanish is the second most widely spoken language in the world as a mother tongue. Official reports, survey-based studies, and Wikipedia confirm it. And Google can predict it.</p><h2><a class=highlight-focus href=/issues/3/on-spanish-parrots/>On Spanish-Speaking Parrots</a></h2><div class=languages aria-label='languages available'><span aria-label=English title="Available in English">EN</span>
<span aria-label=Español title="Disponible en Español">ES</span></div><ul class=authors><li><address>Gimena del Rio Riande</address><a href=/authors/#DelRioRiandeGimena aria-label="author info" title="author info"><i class=ph-link-thin></i></a></li></ul><a class="doi highlight-focus" href=https://doi.org/10.5281/zenodo.6567850>10.5281/zenodo.6567850</a></article><article class=article-summary><div class=bg-text role=presentation><span class=shaper style=height:220px></span>How is it possible to go forward with large language models with the knowledge of just how biased, how incomplete, and how harmful — to both people and the planet — these models truly are?</div><div class="bg-text secondary" role=presentation><span class=shaper style=height:220px></span>How is it possible to go forward with large language models with the knowledge of just how biased, how incomplete, and how harmful — to both people and the planet — these models truly are?</div><p><span class=shaper style=height:220px></span>How is it possible to go forward with large language models with the knowledge of just how biased, how incomplete, and how harmful — to both people and the planet — these models truly are?</p><h2><a class=highlight-focus href=/issues/3/llm-limit-case/>Are Large Language Models Our Limit Case?</a></h2><div class=languages aria-label='languages available'><span aria-label=English title="Available in English">EN</span></div><ul class=authors><li><address>Lauren Klein</address><a href=/authors/#KleinLauren aria-label="author info" title="author info"><i class=ph-link-thin></i></a></li></ul><a class="doi highlight-focus" href=https://doi.org/10.5281/zenodo.6567985>10.5281/zenodo.6567985</a></article></div></section><section class="grid masthead" id=credits><div class=container><h2>Credits</h2><p><strong>Editor</strong>
Grant Wythoff</p><p><strong>Technical Lead</strong>
Rebecca Sutton Koeser</p><p><strong>UX Designer</strong>
Gissoo Doroudian</p><p><strong>Spanish Editor</strong>
David Rivera</p><p><strong>Manuscript Editing</strong>
Camey VanSant</p></div></section></main><footer><nav aria-label="footer links"><ul><li><a class=highlight-focus href=/about/>About</a></li><li><a class=highlight-focus href=/authors/>Authors</a></li></ul></nav><div class=icons><a class="license highlight-focus" rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons Attribution 4.0 International License" src=/img/logos/license.svg width=38 height=20></a>
<a class=highlight-focus href=/ aria-label=Startwords><div class=logo id=startwords></div></a><a class=highlight-focus href=https://cdh.princeton.edu/ aria-label="The Center for Digital Humanities at Princeton"><div class=logo id=cdh></div></a></div></footer></body></html>