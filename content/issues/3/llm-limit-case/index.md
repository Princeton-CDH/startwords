---
type: article
slug: 'llm-limit-case'
title: |
  Are Large Language Models our Limit Case?
order: 3
authors:
    - Lauren Klein
date: 2022-02-01
doi: 10.5281/zenodo.5750691 #TODO: replace DOI
pdf: https://zenodo.org/record/5750691/files/startwords-2-datas-destinations.pdf #TODO: replace PDF
images: ["issues/2/datas-destinations/images/datas-destinations-social.png"] #TODO: replace social image
summary: How is it possible to go forward with large language models with the knowledge of just how biased, how incomplete, and how harmful—to both people and the planet—these models truly are?
hook_height_override: 215
---

How is it possible to go forward with the development and use of large language models, given the clear evidence of just how biased, how incomplete, and how harmful---to both people and the planet---these models truly are? This is the question that "Stochastic Parrots" productively sets in motion, and that has continued to reverberate throughout the artificial intelligence (AI) and machine learning (ML) community since the paper's release.[^1]

This is also a question---or, rather, a more specific version of a question---that I've been thinking through for a number of years. In [*Data Feminism*](https://data-feminism.mitpress.mit.edu/), for example, Catherine D'Ignazio and I ask more broadly how datasets and data systems, which so often encode and amplify power differentials, might instead be reimagined so that they can challenge and rebalance those differentials.[^2] We document myriad instances of biased and incomplete datasets, as well as harmful and oppressive data systems that were conceived without attention to---let alone the involvement of---the communities most impacted by those systems.[^3] We also consider the environmental and economic costs of the computing infrastructure required to support such systems.[^4] Yet we maintain a guarded optimism. Throughout the book, we advance a view that it still may be possible to remake the field of data science by building coalitions across communities and taking differential power into account, so as to wield the power of data with intention and care, and in the service of justice. The [seven principles of data feminism](https://data-feminism.mitpress.mit.edu/pub/frfa9szd/release/6#ebrc4pjetw) that we describe in the book---examine power, challenge power, rethink binaries and hierarchies, elevate emotion and embodiment, embrace pluralism, consider context, and make labor visible---were intended to structure this transformative work.

But when "Stochastic Parrots" was released, and when Dr. Gebru and Dr. Mitchell were [subsequently](https://www.nytimes.com/2020/12/03/technology/google-researcher-timnit-gebru.html) [fired](https://www.nytimes.com/2021/02/19/technology/google-ethical-artificial-intelligence-team.html)---and then [subjected to so much harassment and defamation online](https://www.theverge.com/22309962/timnit-gebru-google-harassment-campaign-jeff-dean)---it called into question the degree to which the transformative data science that Catherine and I had described in the book was truly possible.[^5] With the opportunity to reengage with "Stochastic Parrots," as this roundtable has invited us to do, I've returned to this question of degree, which---one year later---now seems more accurately described as a question of bound: are large language models (LLMs) our limit case? In other words, are LLMs simply too big, and too ethically, environmentally, and epistemologically compromised, for humanities scholars to abide?

I will admit that I do not yet have a definitive answer to this question. But as a way of working through my current thinking, I will offer three unequivocal assertions:

1. **There is no outside of unequal power.**
2. **All technologies are imbricated in this unequal power.**
3. **Refusal is, in itself, a generative act.**

In what follows, I elaborate each of these points.

***********

**First, there is no outside of unequal power.** This This is why an attention to power matters so much for discussions of data and models: it overdetermines not only the data we can collect and the datasets we can access, but also the research questions that we can explore. In *Data Feminism*, Catherine and I demonstrate how the financial and computational resources required to collect and analyze data at scale results in efforts most often undertaken by large corporations (and other well-resourced institutions) for their own profit and benefit, and at the expense of everyone else. This is why, to take one famous example that we discuss in the book, Target is able to analyze its own customer data in order to predict whether or not a person is pregnant---and then use that same data to sell them baby products; but there is not enough actual medical data that can be analyzed in order to predict whether that same pregnant person will be at risk of dying in childbirth.[^6] Or, to take an example discussed in "Stochastic Parrots," why the Colossal Clean Crawled Corpus, even though it is less explicitly sexist, racist, and xenophobic than the CommonCrawl corpus from which it is derived, remains sexist and racist and xenophobic in more subtle ways: because it was created by filtering out documents from the CommonCrawl corpus that contained one or more words drawn from a list of "[Dirty, Naughty, Obscene, or Otherwise Bad Words](https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words)," it also excludes all discussions *about* those words---including direct critiques of those words or, in certain cases, discussions by those who might want to reclaim them. More recent research by Gururangan et al. has shown that even more sophisticated quality filters, such as the classifier employed to cull the dataset on which GPT-3 is trained, demonstrate significant stylistic as well as thematic preferences, preferences which correlate with a number of proxies for high socioeconomic status.[^7]

Literary and historical corpora are not exempt from the influence of unequal power. We all know (or should) that the [HathiTrust corpus](https://www.hathitrust.org/), because it is drawn from the collections of major research libraries, reflects the collecting preferences of those libraries (and the preferences of several libraries in particular) rather than literary production writ large.[^8] More domain-specific corpora, which would seem to avoid some of these issues as a result of their narrower scope, nonetheless continue to fall prey to the politics of digitization, as Katherine Bode and others have astutely observed.[^9] And those of us who work on texts related to the history of slavery, in particular, have long had to reckon with the politics of the archive itself: the fact that the first-hand accounts that might offer us the most direct access to the lives of the enslaved\--because of the historical structures of power that control not only what enters the archive but also who is authorized to even write\--scarcely exist at all. That there is no outside of this unequal power is the starting point for our work. We must build our datasets and our models with our eyes wide open to this fact, and as we acknowledge what we might learn from any new analyses, we must continue to account for the archive's "null values," as Jessica Marie Johnson describes them: spaces held open for the people and stories that we know to have existed in their own time, even as they remain unknown to us in the present.[^10]

Which brings me to my second point: **all technologies are imbricated in this unequal power**. Just as literary and other humanities scholars are deeply attuned to how power shapes both datasets and models, so too are we aware of how technology---all technology---is shaped by power as well. One could point to broad histories of [computing](https://mitpress.mit.edu/books/programmed-inequality), [statistics](https://www.press.uillinois.edu/books/?id=p080241), or [surveillance](https://www.dukeupress.edu/dark-matters) for evidence of this fact.[^11] But we could also look to work that has explored the imbrications of power in specific natural language processing (NLP) and ML techniques themselves. [Work by Jeff Binder](https://dhdebates.gc.cuny.edu/read/untitled/section/4b276a04-c110-4cba-b93d-4ded8fcfafc9), for example, has located the origins of topic modeling in the need for US intelligence agencies to quickly scan international newswires for potential geopolitical conflicts.[^12] [Melanie Walsh, for another, has recently reminded us](https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/12-Named-Entity-Recognition.html) that the annotated [OntoNotes corpus](https://catalog.ldc.upenn.edu/LDC2013T19), on which [spaCy's language models](https://spacy.io/) were trained, was developed with funding by DARPA---as is so much of NLP/ML/AI work today.[^13] This is circles back to a point that Catherine and I make in *Data Feminism*, about unequal power being the result of unequal resources. This point is made in "Stochastic Parrots" as well: because these technologies are so resource-dependent, in terms of not only energy, cost, and computing power, but also because of very specific technical expertise, they are necessarily developed by people at elite and well-resourced institutions who are rarely required---either by inclination or circumstance---to take power into account.

Meredith Whittaker, another former Google employee [forced out because of her labor organizing efforts there](https://www.wired.com/story/google-walkout-organizers-say-theyre-facing-retaliation/), observes as much in [a recent essay](https://interactions.acm.org/archive/view/november-december-2021/the-steep-cost-of-capture) that documents the "capture" of supervised machine learning algorithms (including language models) by big tech. Whittaker explains how any gains in performance they might achieve are the result not of any major algorithmic or architectural innovation, but rather, of what existing algorithms can do "when matched with large-scale data and computational resources."[^14] These are resources that only big tech firms control. So when academic researchers seek to engage with these developments, they find themselves beholden to the very same tech firms for the data and computing infrastructure, and very often funding, as a necessary precondition for contributing on an equal plane. The systematic defunding of higher education that has taken place over the past several decades is not the focus of Whittaker's piece, but this issue is the other side of the same neoliberal coin. Academic researchers, especially at public institutions, are functionally if not ideologically compelled to concede to this asymmetrical configuration of power and resources as a result of the federal, state, and institutional policies that have transferred the responsibility of supporting and sustaining research (in terms of computing infrastructure, student support, and even their own salaries) to the scholars themselves. And all of this is to say nothing of corporations like Facebook that are actively and intentionally wielding their data and algorithms to retain their own power, even as they know full-well the harms that their products produce.[^15]

Given this ethical, intellectual, and economic double-bind, it's not surprising that the most clear and compelling response may be to refuse---to refuse to develop new models, to refuse to improve existing ones, or even to refuse to participate in this work altogether. This brings me to my third assertion: that **refusal is, in itself, a generative act**. I've long admired the work of Dr. Joy Buolamwini and the [Algorithmic Justice League](https://www.ajl.org/), for example. This work began (in work coauthored with Dr. Gebru) by identifying biases in the image datasets used to train three major gender classification software libraries.[^16] But when [the initial audit found significant error rates](http://gendershades.org/) in how the software classified images of women, and images of dark-skinned women in particular, Buolamwini's response was *not* to suggest that the training data be "debiased" or otherwise improved. Rather, because she recognized how improved gender classification software (and facial recognition software more generally) would most likely be used to increase the policing and surveillance of Black and brown people, she used the evidence of her paper with Gebru to instead call for [a ban on facial recognition software altogether](https://www.ajl.org/library/policy-advocacy).[^17]

In terms of text analysis tools more specifically, I've followed with interest [the recent actions by the team behind ml5.js](https://twitter.com/ml5js/status/1445762321444315147), the JavaScript-based machine learning library, which upon discovering racist language in its sample word2vec model, around which its documentation is based, decided to remove it (and as a result, render the entire library non-functional) until an alternate model could be trained. In both of these cases, the AJL and ML5.js, we see evidence of how refusal---especially when accompanied ([as feminists advise](http://www.kanarinka.com/wp-content/uploads/2021/01/Garcia-et-al.-2020-No-Critical-Refusal-as-Feminist-Data-Practice.pdf)) by a recommitment to values, or action, or both---can clear the space to imagine alternate possibilities.[^18]

"Stochastic Parrots" participates in this reimagining by describing an alternative approach to technical research, one in which issues of cost, access, potential harms, and potential benefits, are addressed early in the research process. This slower and more intentional process also allows for input from---and, ideally, meaningful collaboration with---impacted communities. This process echoes some of what Catherine and I have written about in terms of a model of [data science for good vs. data science for co-liberation](https://data-feminism.mitpress.mit.edu/pub/2wu7aft8/release/3#nobxi408tlj): what it would look like to imagine a way of doing data science in which those from both dominant and minoritized groups work together to free themselves from the oppressive systems that harm all of us. But the example at the center of "Stochastic Parrots" complicates this vision in necessary ways, because LLMs may well function as a limit case---not only for the AI/ML researchers who would otherwise work to improve them, but also for scholars of literature and culture, myself included, who would make use of them in their own quantitative work.

|                                                                                                               | "Data for good" | Data for co-liberation |
|:------------------------------------------------------------------------------------------------------------- |:---------------:|:----------------------:|
| Leadership by members of minoritized groups working in community                                              |                 |           x            |
| Money and resources managed by members of minoritized groups                                                  |                 |           x            |
| Data owned and governed by the community                                                                      |                 |           x            |
| Quantitative data analysis “ground truthed” through a participatory, community-centered data analysis process |                 |           x            |
| Data scientists are not rock stars and wizards, but rather facilitators and guides                            |                 |           x            |
| Data education and knowledge transfer are part of the project design                                          |                 |           x            |
| Building social infrastructure—community solidarity and shared understanding—is part of the project design    |                 |           x            |

*Above: “Features of ‘data for good’ versus data for co-liberation, from Catherine D’Ignazio and Lauren F. Klein,* Data Feminism *(MIT Press, 2020), p. 140.*

Quantitative literary and cultural studies scholars must strongly contend with these same ethical, environmental, and economic concerns. But they must also contend with an additional set of concerns---part methodological and part epistemological---that pertain to humanistic inquiry in particular. For example, even as there begin to exist [LLMs that are trained on historical corpora](https://macberth.netlify.app/), the amount of data that is required results in training datasets with timespans---1450 to 1950, in the case of MacBERTh---that far exceed any disciplinary sense of periodization. How can we reconcile the historical specificity that we so value in our own research with the fact that even the most appropriate LLM for historical scholarship may be trained on data so temporally distant from the time period that bounds our own scholarly expertise? Furthermore, even as we know to fine-tune such a model on our own more curated datasets, how are we to measure the effects of that fine-tuning in ways that are meaningful to us as humanities scholars? When parameters no longer correspond to specific textual or linguistic features, as with earlier model architectures, we will require even more creative ways to understand the significance of the texts contained in our curated datasets in relation to those on which the larger model was trained.

In addition, we must consider how decades of feminist thinking---and, for that matter, much of the most profound of humanities scholarship---has confirmed how a single voice at the margins can tell us just as much as (if not more than) a large group at the center. How do we hold fast to this fact as the allure of LLMs, enlisted as they are in the service of "shared" or generalizable tasks, continues to mount? How can we envision methods to engage these models in ways that center marginalized voices and the texts that document them? How can we amplify rather than merely assimilate the important oppositional ideas that these texts record? And how can we do so while remaining mindful of the ideas---and the people behind them---that these models cannot or at times should not subsume?

And so I join the authors of "Stochastic Parrots" in the work of reimagining how we design, develop, and deploy large language models, underscoring the importance of a constant and continual interrogation of whether the benefits of any particular modeling approach outweighs its costs. To this I would add that, as quantitative literary scholars, we must recommit to showing how literary, cultural, and historical context not only enrichs our present understanding of LLMs, but is in fact necessary to all future research. After all, this is the set of contexts from which LLMs emerged, and it is only with a deep knowledge of these contexts that we can truly understand the uses of LLMs and their limits.

[^1]: On "community" as an empty signifier, especially in the AI ethics space, see J. Khadijah Abdurahman, "Holding to Account: Safiya Umoja Noble and Meredith Whittaker on Duties of Care and Resistance to Big Tech," *Logic Magazine* 15 (December 2021). <https://logicmag.io/beacons/holding-to-account-safiya-umoja-noble-and-meredith-whittaker/>

[^2]: Catherine D'Ignazio and Lauren F. Klein, *Data Feminism* (Cambridge: MIT Press, 2020). <https://data-feminism.mitpress.mit.edu/>

[^3]: See "[The Power Chapter](https://data-feminism.mitpress.mit.edu/pub/vi8obxh7/release/4?readingCollection=0cd867ef)" and "[Unicorns, Janitors, Ninjas, Wizards, and Rock Stars](https://data-feminism.mitpress.mit.edu/pub/2wu7aft8/release/3?readingCollection=0cd867ef)" in *Data Feminism.*

[^4]: See "[The Power Chapter](https://data-feminism.mitpress.mit.edu/pub/vi8obxh7/release/4?readingCollection=0cd867ef)" and "[Show Your Work](https://data-feminism.mitpress.mit.edu/pub/0vgzaln4/release/3?readingCollection=0cd867ef)" in *Data Feminism.*

[^5]: For Catherine's response to Dr. Gebru's firing in particular, see, Katlyn Turner et al., "The Abuse and Misogynoir Playbook," *The State of AI Ethics Report* (Montreal AI Ethics Institute, 2021). <https://montrealethics.ai/wp-content/uploads/2021/01/The-State-of-AI-Ethics-Report-January-2021.pdf#page=15>

[^6]: *Data Feminism*, pp. 29-34, 39-41.

[^7]: Suchin Gururangan et al., "Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection" (2022). <https://arxiv.org/pdf/2201.10474.pdf>

[^8]: Benjamin Schmidt, "What's in the HathiTrust?" *Sapping Attention*, February 3, 2022. <https://sappingattention.blogspot.com/2019/03/whats-in-hathi-trust.html>

[^9]: Katherine Bode, "Why You Can't Model Away Bias," *Modern Language Quarterly* 81.1 (2020): 95-124. For an example and analysis of the politics of digitization in action, see Benjamin Fagan, "Chronicling White America," *American Periodicals* 26.1 (2016): 10-13.

[^10]: Jessica Marie Johnson, *Wicked Flesh: Black Women, Intimacy, and Freedom in the Atlantic World* (Philadelphia: Univ. of Pennsylvania Press, 2020).

[^11]: Mar Hicks, *Programmed Inequality: How Britain Discarded Women Technologists and Lost Its Edge in Computing* (MIT Press, 2017); Banu Subramaniam, *Ghost Stories for Darwin: The Science of Variation and the Politics of Diversity* (Univ. of Illinois Press, 2014); Simone Browne, *Dark Matters: On the Surveillance of Blackness* (Duke Univ. Press, 2015).

[^12]: Jeffrey Binder, "Alien Reading: Text Mining, Language Standardization, and the Humanities," *Debates in the Digital Humanities 2016*, ed. Matthew K. Gold and Lauren F. Klein (Minneapolis: Univ. of Minnesota Press). <https://dhdebates.gc.cuny.edu/read/untitled/section/4b276a04-c110-4cba-b93d-4ded8fcfafc9>

[^13]: Melanie Walsh, *Introduction to Cultural Analytics and Python,* Version 1 (2021). <https://doi.org/10.5281/zenodo.4411250>.

[^14]: Meredith Whittaker, "The Steep Cost of Capture," *ACM Interactions* 28.6 (November-December 2021): 50. <https://interactions.acm.org/archive/view/november-december-2021/the-steep-cost-of-capture>

[^15]: See, for example, Karen Hao, "She Risked Everything to Expose Facebook. Now She's Telling Her Story," *MIT Technology Review*, July 29, 2021, <https://www.technologyreview.com/2021/07/29/1030260/facebook-whistleblower-sophie-zhang-global-political-manipulation/> ; and Karen Hao, "The Facebook Whistleblower Says Its Algorithms Are Dangerous Here's Why," *MIT Technology Review,* October 5, 2021, <https://www.technologyreview.com/2021/10/05/1036519/facebook-whistleblower-frances-haugen-algorithms/>

[^16]: Joy Buolamwini and Timnit Gebru, "Intersectional Accuracy Disparities in Commercial Gender Classification," *Proceedings of Machine Learning Research* 81 (2018): 77-91.

[^17]: See the "Policy/Advocacy" page on the *Algorithmic Justice League* website, <https://www.ajl.org/library/policy-advocacy>.

[^18]: On feminist refusal, see Marika Cifor et al, Feminist Data Manifest-No (2019). [[https://www.manifestno.com/]{.ul}](https://www.manifestno.com/) and Patricia Garcia et al., "No: Critical Refusal as Feminist Data Practice," [CSCW \'20 Companion: Conference Companion Publication of the 2020 on Computer Supported Cooperative Work and Social Computing](https://dl.acm.org/doi/proceedings/10.1145/3406865) (October 2020): 199--202. <https://doi.org/10.1145/3406865.3419014>.